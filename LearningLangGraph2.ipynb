{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb53d3e-0ccc-46a7-9fe8-769e40a73e1f",
   "metadata": {},
   "source": [
    "# AI Agent with Tools and Memory\n",
    "\n",
    "AI agents are an integral part of our lives ranging from applications like Customer service, automation of business tasks, stock market research, etc. For this tutorial, I will be using Mistral AI, but you could do the same with any LLM provider.\n",
    "\n",
    "I have stored the `MISTRAL_AI_KEY` environment variable in my system. I will also be using *dotenv* module to store secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49f9531-2259-46d9-9641-fa4e9788650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good to go!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('MISTRAL_API_KEY')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"MISTRAL_API_KEY not found in environment variables\")\n",
    "else:\n",
    "    print(\"All Good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ccc98-1a8e-4f42-9d39-08cdc0713fca",
   "metadata": {},
   "source": [
    "In LangGraph, the interaction between user and AI agent is managed through a sequence of messages. The message types include following.\n",
    "- AI Message: These are responses generated by the LLM\n",
    "- System Message: Messages sent by the agent to provide context or additional instructions to the LLM\n",
    "- Human Message: These are input from the user such as questions or commands.\n",
    "\n",
    "## Q&A Agent\n",
    "\n",
    "In this Q&A agent, you will use LLM to provide answers to user questions and also call external API using tool node to fetch real time information.\n",
    "1. First you will accept user input\n",
    "2. You will pass user question to the LLM. LLM can directly respond or trigger tool nodes if user is asking for current information.\n",
    "3. Tool nodes are used to find the latest information.\n",
    "4. Once the response has been generated, it is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45134cb-95a8-49a8-955b-cc38ba893b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the capital of Kenya?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The capital of Kenya is **Nairobi**. It is the largest city in Kenya and serves as the country's political, economic, and cultural hub.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "model = ChatMistralAI(model='mistral-large-latest', api_key=api_key)\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages[-1].content)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "input_message = {\n",
    "    'messages': [('human', 'What is the capital of Kenya?')]\n",
    "}\n",
    "for chunk in app.stream(input_message, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11bcb4-7cbd-4930-838b-e8a198a2b909",
   "metadata": {},
   "source": [
    "The `call_llm` function sends the user input to the LLM model and returns the generated response.\n",
    "Next, we want to be able to handle continuous user input with multiple question answers in a single session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6b8637-d51c-4317-a4c9-cd29e829ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "model = ChatMistralAI(model='mistral-large-latest', api_key=api_key)\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages[-1].content)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "def interact_with_agent():\n",
    "    while True: \n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending the conversation\")\n",
    "            break\n",
    "        input_message = {\n",
    "            'messages': [('human', user_input)]\n",
    "        }\n",
    "        \n",
    "        for chunk in app.stream(input_message, stream_mode='values'):\n",
    "            chunk['messages'][-1].pretty_print()\n",
    "# interact_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d0ca4-ba74-4222-9923-5923b004e646",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "LLM is great at generating responses based on language understanding, however, it has limitations like it lacks real-time information and cannot perform specific tasks like calling an API or running calculations. Tools allow an AI agent to fetch real-time data, perform specific tasks and retrieve information from databases or external APIs. `ToolNode` is used for calling external tools and integrate it into existing agent.\n",
    "\n",
    "LangChain provides a convenient way to define tools using the `@tool` decorator which turns any Python function into callable tools. In below code, `@tool` decorator makes the function as a tool that can be used within LangGraph. The `get_weather` function takes location as input and returns the weather for that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a035eb7c-d3a5-4e21-af4d-f87d895f91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"\n",
    "    Fetch the current weather for a specific location\n",
    "    \"\"\"\n",
    "    weather_data = {\n",
    "        \"San Francisco\": \"It's 60 degrees and foggy.\",\n",
    "        \"New York\": \"It's 90 degrees and sunny.\",\n",
    "        \"London\": \"It's 70 degrees and cloudy.\"\n",
    "    }\n",
    "    return weather_data.get(location, \"Weather information is unavailable for this location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e95d7-a162-4bbc-a37f-844f094f731e",
   "metadata": {},
   "source": [
    "Once you've tool for fetching the weather information, you need to integrate it into the agent. LangGraph provides a node type called `ToolNode` which is responsible for calling external tools. It takes a list of tools as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7a94ae-38bf-4594-b352-916bcfbb9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "tool_node = ToolNode([get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882cfedb-a9fe-4c2b-ab7e-7a571e2de004",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatMistralAI(model='mistral-large-latest', api_key=api_key).bind_tools([get_weather])\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages[-1].content)\n",
    "    if response.tool_calls:\n",
    "        tool_result = tool_node.invoke({'messages': [response]})\n",
    "        tool_message = tool_result['messages'][-1].content\n",
    "        response.content += f\"\\nTool Result: {tool_message}\"\n",
    "        return {'messages': [response]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b170be3-2c1d-4e62-b705-5ac65ff30c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_agent():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending the conversation\")\n",
    "            break\n",
    "        input_message = {\n",
    "            \"messages\": [(\"human\", user_input)]\n",
    "        }\n",
    "        for chunk in app.stream(input_message, stream_mode='values'):\n",
    "            chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8f5048-51f3-4884-b450-a1031d964b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6f79-e410-4b0c-b098-46cf6774af26",
   "metadata": {},
   "source": [
    "When user asks a question, LLM node processes the input and it generates a response. If the response contains a *tool call*, the graph triggers the `ToolNode` and calls specified tool and returns the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bd864-3ed2-462a-bca4-88928a8c303a",
   "metadata": {},
   "source": [
    "## Error Handling in Tools\n",
    "\n",
    "LangGraph provides built-in error handling for tool calls. If something goes wrong during tool execution, LangGraph will handle the error and return a meaningful message to the user. You can customize the error handling by configuring the `ToolNode` to handle or propagate errors.\n",
    "\n",
    "```python\n",
    "# ToolNode with error handling disabled (propagating errors to the user)\n",
    "tool_node = ToolNode([get_weather], handle_tool_errors=False)\n",
    "```\n",
    "\n",
    "If the weather tool encounters an error, the agent will let the user konw that something went wrong rather than silently handling the error.\n",
    "\n",
    "## Understanding Tool call\n",
    "\n",
    "In this case, you will mimic tool calling for getting user profile. It includes following steps.\n",
    "1. Define your tool using a Python function with `@tool` decorator. In this case, I have defined `get_user_profile` function which can retrieve user profile using `user_id`.\n",
    "2. Next, set up `ToolNode` for calling the `get_user_profile` tool when AI agent asks for it.\n",
    "3. In order to mimic tool calling, define `AIMessage` which tells the AI agent to call specific tool and provides required input.\n",
    "4. Next, set up the `StateGraph`. In this case, the `StateGraph` must have `messages` key.\n",
    "5. Invoke the tool using `ToolNode` to process the state. The `ToolNode` will look at the last message in the state, find the tool call and execute the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3db907-8644-4172-8a51-5455f2dd6abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [ToolMessage(content='{\"name\": \"Alice\", \"age\": 30, \"location\": \"New York\"}', name='get_user_profile', tool_call_id='tool_call_id')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# 1. Define the tool\n",
    "@tool\n",
    "def get_user_profile(user_id: str):\n",
    "    \"\"\"\n",
    "    Fetch the profile of a user by user ID\n",
    "    \"\"\"\n",
    "    user_data = {\n",
    "        \"101\": {\"name\": \"Alice\", \"age\": 30, \"location\": \"New York\"},\n",
    "        \"102\": {\"name\": \"Bob\", \"age\": 25, \"location\": \"San Francisco\"}\n",
    "    }\n",
    "    return user_data.get(user_id, \"User profile not found.\")\n",
    "\n",
    "# 2. Setup ToolNode\n",
    "tools = [get_user_profile]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 3. Set up AIMessage for tool calling\n",
    "message_with_tool_call = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"name\": \"get_user_profile\",\n",
    "            \"args\": {\"user_id\": \"101\"},\n",
    "            \"id\": \"tool_call_id\",\n",
    "            \"type\": \"tool_call\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Set up StateGraph\n",
    "state = {\n",
    "    \"messages\": [message_with_tool_call]\n",
    "}\n",
    "\n",
    "# 5. Invoke the ToolNode with state\n",
    "result = tool_node.invoke(state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963b3fc-b345-4f38-988a-7e9890d4884b",
   "metadata": {},
   "source": [
    "## Adding Memory\n",
    "\n",
    "Short-term memory helps an agent maintain context during a conversation within a session but not across multiple sessions, making it more coherent.\n",
    "\n",
    "### Agent without Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf3fcb4-c5f6-4dce-a323-de22a730d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "model = ChatMistralAI(model='mistral-large-latest', api_key=api_key)\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages[-1].content)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "def interact_with_agent():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending the conversation.\")\n",
    "            break\n",
    "        input_message = {\n",
    "            'messages': [('human', user_input)]\n",
    "        }\n",
    "        for chunk in app.stream(input_message, stream_mode='values'):\n",
    "            chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac79374f-a81b-4f63-82c6-ba8070606b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  My name is Jenny Zim.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is Jenny Zim.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, Jenny Zim! How can I assist you today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I donâ€™t have access to your name unless youâ€™ve shared it with me in our conversation. If youâ€™d like, you can tell me your name, and Iâ€™ll remember it for this chat! ðŸ˜Š\n",
      "\n",
      "What should I call you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the conversation.\n"
     ]
    }
   ],
   "source": [
    "# interact_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ee935-969e-4900-8c03-ba423ab70fce",
   "metadata": {},
   "source": [
    "### Agent with Short-term Memory\n",
    "\n",
    "With short-term meomry, the agent can remember the conversation during the session but will forget everything once the session ends. LangGraph offers `MemorySaver` for implementing short-term memory easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33c2cc41-3a62-4720-9d6d-40f4117274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "def interact_with_agent_with_memory():\n",
    "    thread_id = 'session_1'\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending the conversation.\")\n",
    "            break\n",
    "        input_message = {\n",
    "            'messages': [('human', user_input)]\n",
    "        }\n",
    "        config = {'configurable': {'thread_id': thread_id}}\n",
    "        for chunk in app_with_memory.stream(input_message, config=config, stream_mode='values'):\n",
    "            chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b922407-a017-4152-849f-ad55edd60d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  My name is Jenny Zim.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is Jenny Zim.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, Jenny Zim! How can I assist you today? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is **Jenny Zim**â€”nice to \"meet\" you again! ðŸ˜Š Let me know if thereâ€™s anything I can help with.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the conversation.\n"
     ]
    }
   ],
   "source": [
    "# interact_with_agent_with_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b2fab0-e0f1-4f7d-b38e-16838372f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Alex! ðŸ˜Š Nice to meet you! Howâ€™s your day going so far? Is there anything I can help you withâ€”whether itâ€™s answering questions, brainstorming ideas, or just chatting? Let me know! ðŸš€\n",
      "\n",
      "(Also, fun fact: \"Alex\" is a great nameâ€”itâ€™s got that classic yet versatile vibe! ðŸ‘Œ)\n",
      "Your name is **Alex**â€”just like you introduced yourself! ðŸ˜„\n",
      "\n",
      "(Though if you ever want to go by a different name or nickname, just let me knowâ€”Iâ€™m happy to adjust!) ðŸ‘‹âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "# Define a config with a thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "model = ChatMistralAI(model='mistral-medium-latest')\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Define the structure of the state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Define the logic\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build the Graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_edge(\"chatbot\", END)\n",
    "\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "# First interaction\n",
    "user_input = \"Hi, my name is Alex.\"\n",
    "result = app_with_memory.invoke({\"messages\": [HumanMessage(content=user_input)]}, config)\n",
    "print(result[\"messages\"][-1].content)\n",
    "\n",
    "# Second interaction (memory recalls the name)\n",
    "user_input = \"What is my name?\"\n",
    "result = app_with_memory.invoke({\"messages\": [HumanMessage(content=user_input)]}, config)\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30a90f-77ee-455a-ba5c-74dba9d87a28",
   "metadata": {},
   "source": [
    "With `MemorySaver`, you can store the state of the conversation within the session. LangGraph saves checkpoints (snapshots of the conversation state) at every step and linked to a thread ID which simulates a session. Once the session ends, memory is discarded.\n",
    "\n",
    "### Memory Across multiple sessions\n",
    "\n",
    "LangGraph uses checkpointers and thread IDs to store the state of the conversation at every interaction (super-step). Thread IDs uniquely identify a session or conversation, allowing the agent to restore the conversation from a previous checkpoint when the same thread ID is provided. With this information, you can modify existing agent to persist memory between different sessions by using thread IDs to link conversations. When the program ends the data in RAM will be lost since session are in-memory at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc8679f4-fd2b-4b8e-b854-6cc424a07faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "model = ChatMistralAI(model='mistral-medium-latest', api_key=api_key)\n",
    "\n",
    "def call_llm(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node('call_llm', call_llm)\n",
    "workflow.add_edge(START, 'call_llm')\n",
    "workflow.add_edge('call_llm', END)\n",
    "app_with_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "def interact_with_agent_across_sessions():\n",
    "    while True:\n",
    "        thread_id = input(\"Enter thread ID (or 'new' for a new session): \")\n",
    "        if thread_id.lower() in ['quit', 'exit', 'end_session']:\n",
    "            print(\"Ending the conversation.\")\n",
    "            break\n",
    "        if thread_id.lower() == 'new':\n",
    "            thread_id = f\"session_{os.urandom(4).hex()}\"\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() in ['exit', 'quit']:\n",
    "                print(f\"Ending the session {thread_id}.\")\n",
    "                break\n",
    "            input_message = {\n",
    "                'messages': [('human', user_input)]\n",
    "            }\n",
    "            config = {'configurable': {'thread_id': thread_id}}\n",
    "            for chunk in app_with_memory.stream(input_message, config=config, stream_mode='values'):\n",
    "                chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287f8277-a767-492b-bef9-348db9578230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter thread ID (or 'new' for a new session):  new\n",
      "You:  My name Jack Daniel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name Jack Daniel.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Nice to meet you, **Jack Daniel**! Thatâ€™s a strong and classic nameâ€”like the famous whiskey (though Iâ€™m sure youâ€™ve heard that before).\n",
      "\n",
      "How can I help you today? Need advice, trivia, or just a fun chat? Let me know! ðŸ¥ƒ (Or notâ€”your call.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is my last name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my last name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on your introduction, your last name is **Daniel**â€”so your full name is **Jack Daniel**.\n",
      "\n",
      "(Though if you're referring to the whiskey brand, itâ€™s *Jack Danielâ€™s*, with an apostrophe, named after its founder, Jasper Newton \"Jack\" Daniel.)\n",
      "\n",
      "Need help with something else? ðŸ˜Š\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the session session_e7f7d3ea.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter thread ID (or 'new' for a new session):  session_e7f7d3ea\n",
      "You:  What is my first name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my first name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first name is **Jack**â€”as in **Jack Daniel**!\n",
      "\n",
      "(And if you ever want to switch things up, you could go by *J.D.* for a cool initial vibe. ðŸ˜Ž) Let me know if you'd like help with anything else!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the session session_e7f7d3ea.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter thread ID (or 'new' for a new session):  new\n",
      "You:  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I donâ€™t have access to personal information about you unless you share it with me! If youâ€™d like, you can tell me your name, and Iâ€™ll be happy to use it in our conversation. ðŸ˜Š\n",
      "\n",
      "(Or if this is a fun riddleâ€”let me know, and Iâ€™ll play along!)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the session session_b370185d.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter thread ID (or 'new' for a new session):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the conversation.\n"
     ]
    }
   ],
   "source": [
    "# interact_with_agent_across_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4d7f8-e934-4cc9-bb00-074f245e7448",
   "metadata": {},
   "source": [
    "LangGraph saves the state of the conversation at every interaction as a checkpoint with each checkpoint containing the conversation context. Every session is associated with a thread ID. When the same thread ID is reused, LangGraph restores the conversation context from the last checkpoint associated with that thread.\n",
    "\n",
    "Sometimes, it's necessary for the agent to remember certain user information like settings, personal data across all sessions even when a new thread is started. For this, LangGraph provides the `MemoryStore`. `MemoryStore` allows the agent to store information that can be shared across different sessions for the same user. For example, the agent could store user's preferred language or settings which would persist across all future conversations regardlesss of the session ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5e7d0b-7581-411f-9989-1c824e406d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember my name is Alice\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "User information saved.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello alice, welcome back!\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "model = ChatMistralAI(model='mistral-medium-latest', api_key=api_key)\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "def store_user_info(state: MessagesState, config, *, store=in_memory_store):\n",
    "    user_id = config['configurable']['user_id']\n",
    "    namespace = (user_id, 'memories')\n",
    "    # Create memory based on the conversation memory_id\n",
    "    memory_id = str(uuid.uuid4())\n",
    "    memory = {'user_name': state['user_name']}\n",
    "    # Save the memory to the in-memory store\n",
    "    store.put(namespace, memory_id, memory)\n",
    "    return {'messages': ['User information saved.']}\n",
    "\n",
    "def retrieve_user_info(state: MessagesState, config, *, store=in_memory_store):\n",
    "    user_id = config['configurable']['user_id']\n",
    "    namespace = (user_id, 'memories')\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        info = f\"Hello {memories[-1].value['user_name']}, welcome back!\"\n",
    "    else:\n",
    "        info = \"I don't have any information about you yet.\"\n",
    "    return {'messages': [info]}\n",
    "\n",
    "def call_model(state: MessagesState, config):\n",
    "    last_message = state['messages'][-1].content.lower()\n",
    "    if 'remember my name' in last_message:\n",
    "        user_name = last_message.split('remember my name is')[-1].strip()\n",
    "        state['user_name'] = user_name\n",
    "        return store_user_info(state, config)\n",
    "    if \"what's my name\" in last_message or \"what is my name\" in last_message:\n",
    "        # Retrieve user's name from memory\n",
    "        return retrieve_user_info(state, config)\n",
    "    # Default LLM response for other inputs\n",
    "    return {\"messages\": [\"I didn't understand your request.\"]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "\n",
    "app_with_memory = workflow.compile(checkpointer=MemorySaver(), store=in_memory_store)\n",
    "\n",
    "# Simulate sessions\n",
    "def simulate_sessions():\n",
    "    config = {\"configurable\": {\n",
    "        \"thread_id\": \"session_1\",\n",
    "        \"user_id\": \"user_123\"\n",
    "    }}\n",
    "    input_message = {\n",
    "        \"messages\": [\n",
    "            {\"type\": \"user\", \"content\": \"Remember my name is Alice\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for chunk in app_with_memory.stream(input_message, config=config, stream_mode=\"values\"):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    config = {\"configurable\": {\n",
    "        \"thread_id\": \"session_2\",\n",
    "        \"user_id\": \"user_123\"\n",
    "    }}\n",
    "    input_message = {\n",
    "        \"messages\": [\n",
    "            {\"type\": \"user\", \"content\": \"What is my name?\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for chunk in app_with_memory.stream(input_message, config=config, stream_mode=\"values\"):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "simulate_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df4ca5-b9e7-4e6c-ba21-a6a432aba731",
   "metadata": {},
   "source": [
    "### Checkpointers\n",
    "\n",
    "This is responsible for saving the state of a graph at each super-step in the workflow. Each checkpoint is a snapshot of the current graph state and contains crucial information like configuration, metadata and state values. A checkpoint is represented by a `StateSnapshot` object and contains following properties.\n",
    "- `Config` contains configuration associated with the checkpoint including `thread_id` and optional `checkpoint_id`.\n",
    "- `Metadata` provides details about the source of the checkpoint and the graph's progress at this point.\n",
    "- `Values` represent the current state of the channels in the graph at the time the checkpoint was taken.\n",
    "- `Next` is a tuple of the node names to execute next in the graph.\n",
    "- `Tasks` is a tuple of `PregelTask` objects that contain information about the next tasks to execute in the graph. It also holds error data if an execution failed or was interrupted.\n",
    "\n",
    "Each checkpoint represents the state of the graph at a specific super-step and can be replayed or updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d885974d-c535-4c77-b69b-5d4d4e36d742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 'b', 'bar': ['a', 'b']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "    bar: list[str]\n",
    "\n",
    "def node_a(state: State):\n",
    "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
    "\n",
    "def node_b(state: State): \n",
    "    return {\"foo\": \"b\", \"bar\": [\"a\", \"b\"]}\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(node_a)\n",
    "workflow.add_node(node_b)\n",
    "workflow.add_edge(START, \"node_a\")\n",
    "workflow.add_edge(\"node_a\", \"node_b\")\n",
    "workflow.add_edge('node_b', END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"foo\": \"\", \"bar\":[]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56fef2-c586-4e28-b52a-51118e84ef5b",
   "metadata": {},
   "source": [
    "You can retrieve the latest state of the graph by calling `graph.get_state()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef81bb28-8611-4b86-b969-e9ea0489814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'b', 'bar': ['a', 'b']}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "latest_state = graph.get_state(config)\n",
    "print(latest_state.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884ddc96-ad70-4db5-b03b-957e84473dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'b', 'bar': ['a', 'b']}\n",
      "{'foo': 'a', 'bar': ['a']}\n",
      "{'foo': '', 'bar': []}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Get state history\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state_history = graph.get_state_history(config)\n",
    "for snapshot in state_history:\n",
    "    print(snapshot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b3347-cd5d-4a0e-8bff-ed2df00e7e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1302f199-e5e1-4337-8f44-3f8ff1ee3fc7",
   "metadata": {},
   "source": [
    "### InMemoryStore\n",
    "This allows you to persist information across different threads and sessions. While checkpointers are tied to a specific session, the memory store can retain user  informaiton, preferences and history between sessions.\n",
    "- In `InMemoryStore`, memories are saved using a namespace which typically includes a `user_id` to uniquely identify the memory.\n",
    "- It uses `put()` to store a memory and `search()` function to retrieve from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a41097fa-0917-424d-8852-005ce68a3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '6cb5b0fa-85cc-4bca-bb8a-b238f7549b5a', 'value': {'food_preference': 'I like pizza'}, 'created_at': '2026-01-28T02:03:11.691429+00:00', 'updated_at': '2026-01-28T02:03:11.691438+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "# Define user namespace using (user_id + 'memories')\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# store food preferences\n",
    "memory_id = str(uuid.uuid4())\n",
    "memory = {\"food_preference\": \"I like pizza\"}\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory)\n",
    "\n",
    "# Retrieve from memories\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "print(memories[-1].dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2113070-a481-4b03-912d-0ad24a2cc60d",
   "metadata": {},
   "source": [
    "In practical applications, you will often use `checkpointers` for session-based memory and `InMemoryStore` for persistent memory across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972379fc-23c4-4b0a-8057-90ffe4e57b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MemoryState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     11\u001b[39m config = {\n\u001b[32m     12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     13\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msession_1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m             }\n\u001b[32m     16\u001b[39m          }\n\u001b[32m     17\u001b[39m graph.invoke({\u001b[33m\"\u001b[39m\u001b[33mfoo\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m}, config)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_memory\u001b[39m(state: \u001b[43mMemoryState\u001b[49m, config: RunnableConfig, *, store: BaseStore):\n\u001b[32m     20\u001b[39m     user_id = config[\u001b[33m'\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     21\u001b[39m     namespace = (user_id, \u001b[33m\"\u001b[39m\u001b[33mmemories\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MemoryState' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "## Initialize checkpointer and memory store\n",
    "checkpointer = MemorySaver()\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "## Compile the graph with memory and checkpointers\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "## invoke the graph with thread_id and user_id config\n",
    "config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"session_1\",\n",
    "                \"user_id\": \"1\"\n",
    "            }\n",
    "         }\n",
    "graph.invoke({\"foo\": \"\"}, config)\n",
    "\n",
    "def update_memory(state: MemoryState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config['configurable']['user_id']\n",
    "    namespace = (user_id, \"memories\")\n",
    "    # Store a memory\n",
    "    memory_id = str(uuid.uuid4())\n",
    "    store.put(namespace, memory_id, {\"favorite_food\": \"pizza\"})\n",
    "    # Retrieve stored memories \n",
    "    memories = store.search(namespace)\n",
    "    return {\"message\": [f\"I remember you like {memories[-1].value['favorite_food']}\"]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63320092-7d02-42b1-88c3-9b04a2b64133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
