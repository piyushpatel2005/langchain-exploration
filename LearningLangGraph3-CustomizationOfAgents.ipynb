{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bad8bc-35ea-4afb-823f-e5012c759aeb",
   "metadata": {},
   "source": [
    "# Advanced Routing and Customization of AI Agents\n",
    "\n",
    "The ability to direct user input through different paths, workflows or tasks based on some criteria is referred to as **Routing** in AI agents. This is very useful for complex interactions allowing agents to decide which task or service to invoke based on user input. Without routing, the agent will struggle to manage different types of tasks and would often provide incorrect responses. Routing ensures that the agent processes each query through the correct workflow or task.\n",
    "\n",
    "## `tools_condition`\n",
    "\n",
    "LangGraph has prebuilt method for handling conditional routing for tools using `tools_condition` to decide whether to call a tool or respond directly. The `tools_condition` evaluates whether the recent message from assistant involves invoking a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2bc4eb-2f8f-40cc-ad35-308238e0106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "12\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "llm = ChatMistralAI(model='mistral-large-latest')\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Function to multiply two numbers.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Node to call LLM with tools\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Node that calls the LLM with tools bound\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "# Build workflow\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode([multiply]))\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "# Add conditional edge based on tool usage\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\", tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# simulate invocation\n",
    "def simulate(): \n",
    "    user_input = {\n",
    "        \"messages\": [\n",
    "            (\"human\", \"Can you multiply 3 by 4?\")\n",
    "        ]\n",
    "    }\n",
    "    result = graph.invoke(user_input)\n",
    "    return result['messages'][-1].pretty_print()\n",
    "\n",
    "print(simulate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fa588-53bb-45d2-991e-86a31574524e",
   "metadata": {},
   "source": [
    "## Custom Conditional Routing\n",
    "\n",
    "Sometimes, you may need to invoke different tools or node based on the user query. Below example routes weather related queries to a weather node, calculation related queries to calculator node and responds to unknown queries with a default response.\n",
    "1. Define three nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76d425d-3970-4689-b523-f11ccc4661b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "def weather_node(state: MessagesState):\n",
    "    return {\"messages\": [\"It's a beautiful sunny day with 22C temperature\"]}\n",
    "\n",
    "def calculator_node(state: MessagesState):\n",
    "    user_query = state['messages'][-1].content.lower()\n",
    "    if \"add\" in user_query:\n",
    "        numbers = [int(s) for s in user_query.split() if s.isdigit()]\n",
    "        result = sum(numbers)\n",
    "        return {\"messages\": [f\"The result of addition is {result}.\"]}\n",
    "    return {\"messages\": [\"I can only perform addition for now.\"]}\n",
    "\n",
    "def default_node(state: MessagesState):\n",
    "    return {\"messages\": [\"Sorry, I don't understand that request.\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe7fdb-deb7-4705-bd15-c7b2ee574953",
   "metadata": {},
   "source": [
    "2. Next, you need to define the routing function which decides the node where to send user query based on content. The `routing_function` inspects the last user message and routes to weather node if message contains the word 'weather', routes to the `calculator_node` if the mssage contains words like 'add' or 'calculate' and routes to the `default_node` if the input doesn't match any expected conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93757a41-4e36-4ba3-b5af-210654a2af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(state: MessagesState):\n",
    "    last_message = state['messages'][-1].content.lower()\n",
    "    if 'weather' in last_message:\n",
    "        return \"weather_node\"\n",
    "    elif 'add' in last_message or 'calculate' in last_message:\n",
    "        return \"calculator_node\"\n",
    "    return \"default_node\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a38938-2c8c-48bc-af9c-4189cd15efef",
   "metadata": {},
   "source": [
    "3. The next step is to build the workflow graph using conditional edges to dynamically route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6caebbe-970e-4caf-88fe-24431121994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"weather_node\", weather_node)\n",
    "builder.add_node(\"calculator_node\", calculator_node)\n",
    "builder.add_node(\"default_node\", default_node)\n",
    "builder.add_node(\"routing_function\", routing_function)\n",
    "builder.add_conditional_edges(START, routing_function)\n",
    "builder.add_edge(\"weather_node\", END)\n",
    "builder.add_edge(\"calculator_node\", END)\n",
    "builder.add_edge(\"default_node\", END)\n",
    "\n",
    "app = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2c74a-1cdf-448f-b3b9-662bf087f2ba",
   "metadata": {},
   "source": [
    "4. Create a function to simulate teh user interaction for routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17816499-7c3e-444c-9d48-c6eeb9fb1b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is weather like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is weather like?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "It's a beautiful sunny day with 22C temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Add 2 and 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 2 and 3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The result of addition is 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is time?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is time?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Sorry, I don't understand that request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "def simulate_interaction():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        input_message = {\"messages\": [\n",
    "            (\"human\", user_input)\n",
    "        ]}\n",
    "        for result in app.stream(input_message, stream_mode='values'):\n",
    "            result['messages'][-1].pretty_print()\n",
    "\n",
    "# simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea3a6c-59a7-4a40-ba70-b1c10e60e987",
   "metadata": {},
   "source": [
    "Conditional routing allows AI agent to handle multiple types of queries with ease and ensures the agent's behavior remains flexible and responsive.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "In LangGraph, streaming allows for real-time updates of the graph's state or node outputs while the workflow is being executed. This is useful for long running processes or when continuous feedback is necessary. In LangGraph, following strreaming options available.\n",
    "1. *Full state Streaming* streams entire state of the graph after each node execution.\n",
    "2. *Updates Streaming* streams only the updates to the state after each node execution.\n",
    "3. *LLM Token Streaming* streams the tokens produced by the LLM during its generation process.\n",
    "\n",
    "### 1. Full State Streaming\n",
    "This is useful when you want to monitor all the changes in the graph after every node execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113a203f-e147-4828-945c-d90c61f05af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [('human', 'Tell me the weather')]}\n",
      "{'messages': [('human', 'Tell me the weather'), \"It's a beautiful sunny day with 22C temperature\"]}\n",
      "{'messages': [('human', 'Tell me the weather'), \"It's a beautiful sunny day with 22C temperature\", 'I can only perform addition for now.']}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Defint the state schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "llm = ChatMistralAI(model='mistral-large-latest')\n",
    "\n",
    "def weather_node(state: State):\n",
    "    return {\"messages\": [\"It's a beautiful sunny day with 22C temperature\"]}\n",
    "\n",
    "def calculator_node(state: State):\n",
    "    user_query = state['messages'][-1].lower()\n",
    "    if \"add\" in user_query:\n",
    "        numbers = [int(s) for s in user_query.split() if s.isdigit()]\n",
    "        result = sum(numbers)\n",
    "        return {\"messages\": [f\"The result of addition is {result}.\"]}\n",
    "    return {\"messages\": [\"I can only perform addition for now.\"]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"weather_node\", weather_node)\n",
    "builder.add_node(\"calculator_node\", calculator_node)\n",
    "builder.add_edge(START, \"weather_node\")\n",
    "builder.add_edge(\"weather_node\", \"calculator_node\")\n",
    "builder.add_edge(\"calculator_node\", END)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [\n",
    "        (\"human\", \"Tell me the weather\")\n",
    "    ]}\n",
    "    for result in app.stream(input_message, stream_mode='values'):\n",
    "        print(result)\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c5050-b7c1-4add-8bfa-1eda518c5d57",
   "metadata": {},
   "source": [
    "### 2. Updstes Streaming\n",
    "\n",
    "This is efficient when you only need to see what has changed rather than the entire state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e5732b-a60e-4f4a-bdc1-cbfc0d3ee20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weather_node': {'messages': [\"It's a beautiful sunny day with 22C temperature\"]}}\n",
      "{'calculator_node': {'messages': ['I can only perform addition for now.']}}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Defint the state schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "llm = ChatMistralAI(model='mistral-large-latest')\n",
    "\n",
    "def weather_node(state: State):\n",
    "    return {\"messages\": [\"It's a beautiful sunny day with 22C temperature\"]}\n",
    "\n",
    "def calculator_node(state: State):\n",
    "    user_query = state['messages'][-1].lower()\n",
    "    if \"add\" in user_query:\n",
    "        numbers = [int(s) for s in user_query.split() if s.isdigit()]\n",
    "        result = sum(numbers)\n",
    "        return {\"messages\": [f\"The result of addition is {result}.\"]}\n",
    "    return {\"messages\": [\"I can only perform addition for now.\"]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"weather_node\", weather_node)\n",
    "builder.add_node(\"calculator_node\", calculator_node)\n",
    "builder.add_edge(START, \"weather_node\")\n",
    "builder.add_edge(\"weather_node\", \"calculator_node\")\n",
    "builder.add_edge(\"calculator_node\", END)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [\n",
    "        (\"human\", \"Tell me the weather\")\n",
    "    ]}\n",
    "    for result in app.stream(input_message, stream_mode='updates'):\n",
    "        print(result)\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43265d5b-490c-4e82-84a9-3121c072cfc1",
   "metadata": {},
   "source": [
    "### 3. LLM Token Streaming\n",
    "\n",
    "This is useful when using LLM to generate long responses. You can stream the tokens as they are produced rather than waiting for the entire response to be generated. This provides responsiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c43d5e41-da9a-4a67-9049-0b55968238d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of| course!| Hereâ€™s| a long|,| winding| joke for youâ€”|it|â€™s a| classic \"|light| bulb|\" joke with| a lot| of setup|.| Buck|le up!\n",
      "\n",
      "|---\n",
      "\n",
      "|**Why| did it| take| so| long to change| the light| bulb?**\n",
      "\n",
      "Because| the| light bulb was| in| a| haunted| house,| and it| needed| a team| of experts| to change| it.\n",
      "\n",
      "First|, a| **|psych|ologist** came| in| and said|, *\"|This| light| bulb is clearly| suffering| from a fear| of the| dark|.| We| need to address| its| anxiety| before| we| can proceed|.\"*\n",
      "\n",
      "Then|, a **pl|umber** showed| up and| said, *\"The| wiring| in| this| house| is all| wrong|.| The| light bulb is getting| mixed| signals|.| I|â€™ll| need to rer|oute the| electricity|.\"*\n",
      "\n",
      "Next|, a **law|yer** arrived| and said, *\"|Before| we change| the light bulb,| we need to establish| liability|. Who|â€™s| responsible| for the| darkness| in| this house?\"*\n",
      "\n",
      "|Then|, a **car|p|enter** came in| and said, *\"|The fixture| is| too| loose.| I|â€™ll| need to reinforce| the socket| before| we can install| a| new bulb|.\"*\n",
      "\n",
      "After| that|, a **zo|ologist** showed| up and said,| *\"This| house| is inf|ested with ghosts|. We need to| ex|orc|ise them before we| can safely| change the bulb|.\"*\n",
      "\n",
      "Then|, a **chef|** arrived and said|, *\"The light| bulb is too| hot. I|â€™ll need to cool| it| down with| a fan before| we can touch| it.\"*\n",
      "\n",
      "Next|, a **me|chan|ic** came in| and said, *\"|The| bulb| is too tight|. Iâ€™ll need| to lo|osen it| with| a w|rench before we can| remove| it.\"*\n",
      "\n",
      "Then|, a **det|ective** showed| up and said,| *\"This| bulb| was| clearly| murdered|. We need to| solve| the| case| before we can replace| it.\"*\n",
      "\n",
      "Finally,| a **hand|yman** walked| in|,| uns|cre|wed| the old| bulb|, and screwed| in| a| new one.\n",
      "\n",
      "|The| others| stared| in| disbelief.\n",
      "\n",
      "|*\"|How| did you| do that| so| easily|?\"* they| asked|.\n",
      "\n",
      "The| hand|yman shrugged|.| *\"I just| turned| off| the power|.\"*\n",
      "\n",
      "|---\n",
      "\n",
      "|Hope| that| gave| you a good laugh| (|or at| least a groan|)!| Want| another| one|?| ðŸ˜†|"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import asyncio\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AIMessageChunk, HumanMessage\n",
    "\n",
    "# Define the state schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Initialize the LLM\n",
    "model = ChatMistralAI(model=\"mistral-small-latest\")\n",
    "\n",
    "# Define a node to handle LLM queries\n",
    "async def call_llm(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await model.ainvoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_edge(START, \"call_llm\")\n",
    "workflow.add_edge(\"call_llm\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Simulate interaction and stream tokens\n",
    "async def simulate_interaction():\n",
    "    input_message = {\"messages\": [(\"human\", \"Tell me a very long joke\")]}\n",
    "    first = True\n",
    "    # Stream LLM tokens\n",
    "    async for msg, metadata  in app.astream(input_message, stream_mode=\"messages\"):\n",
    "        if msg.content and not isinstance(msg, HumanMessage):\n",
    "            print(msg.content, end=\"|\", flush=True)\n",
    "\n",
    "        if isinstance(msg, AIMessageChunk):\n",
    "            if first:\n",
    "                gathered = msg\n",
    "                first = False\n",
    "            else:\n",
    "                gathered = gathered + msg\n",
    "\n",
    "            if msg.tool_call_chunks:\n",
    "                print(gathered.tool_calls)\n",
    "\n",
    "await simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab232f4b-709b-4839-a9d3-4abf1383e27b",
   "metadata": {},
   "source": [
    "You can combine different streaming modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26401062-e668-4c40-b584-b45ffcf7c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s a long, winding joke for youâ€”itâ€™s a classic \"shaggy dog\" story with a slow build and a silly punchline. Grab a snack, get comfortable, and enjoy!\n",
      "\n",
      "---\n",
      "\n",
      "A man walks into a bar with a tiny alligator by his side. He puts the alligator up on the bar. He turns to the astonished patrons and says, \"I'll make you a deal. I'll open this alligator's mouth and place my private parts inside. Then the gator will close his mouth for one minute. After that, I'll remove my unmentionables unharmed. In return for witnessing this spectacle, I hope that you fine people will buy me a drink.\"\n",
      "\n",
      "The crowd murmurs their approval. The man stands up on the bar, drops his trousers, and places his family jewels into the alligator's open mouth. The gator chomps downâ€”*CLAMP!*â€”and the man grins wildly. The crowd counts aloud to 60, and sure enough, the man removes his bits, totally unscathed. The crowd cheers, buys him drinks, and asks how he does it.\n",
      "\n",
      "\"Simple,\" he says. \"The gator is a professional, and I have a great relationship with him. I'll even offer you fine folks a chance to try it yourselves with my alligatorâ€”*for $100*.\"\n",
      "\n",
      "A hush falls over the crowd. After a moment, a hand goes up in the back. A burly man stands and says, \"I'll try, but you have to promise that if anything goes wrong, you'll pay for all my medical bills.\"\n",
      "\n",
      "\"Deal!\" says the alligator guy. The burly man drops his pants, places his goods in the gator's mouth, andâ€”*CHOMP!*â€”the alligator slams its jaws shut. The crowd counts to 30... then 40... then 50... The burly man's face turns purple. At 55 seconds, he faints. At 60 seconds, the alligator guy prys open the gator's mouth, and the burly man's privates come outâ€”*completely mangled*.\n",
      "\n",
      "The crowd gasps in horror. The alligator guy sighs and says, \"Well, that's never happened before. Here's $1,000 for your medical bills.\" He helps the burly man to his feet, and the poor guy stumbles out, groaning in pain.\n",
      "\n",
      "The crowd sits in stunned silence. Finally, a woman in the corner raises her hand and says, \"I've got a question. Where do you find an alligator like that?\"\n",
      "\n",
      "The man replies, \"Ah, thatâ€™s easy. Thereâ€™s a guy a few blocks from here who breeds them. Iâ€™ll take you there if you want.\"\n",
      "\n",
      "The woman agrees, and they leave the bar. They walk a few blocks until they come to a run-down pet shop with a sign that reads: **\"EXOTIC PETSâ€”ALLIGATORS, SNAKES, SPIDERS, ETC.\"**\n",
      "\n",
      "They go inside. Itâ€™s dimly lit, and the air smells like damp hay. The alligator guy calls out, \"Yo, Frank! You here?\"\n",
      "\n",
      "A voice from the back yells, \"Hold on!\" A moment later, a grizzled old man in a stained apron shuffles out, holding a tiny, legless alligator in his hands. He squints at the visitors and says, \"Whaddya want?\"\n",
      "\n",
      "The alligator guy gestures to the woman and says, \"This lady wants to know where you get alligators like *mine*.\"\n",
      "\n",
      "Frank looks confused. \"Like *yours*? Buddy, I *sold* you that alligator. That was *three years ago*.\"\n",
      "\n",
      "The alligator guy stares at Frank, then at the tiny, legless alligator in Frankâ€™s hands. His face pales. He turns to the woman and says, **\"Oh no... I think I bought the *wrong* pet.\"**\n",
      "\n",
      "---\n",
      "\n",
      "**Moral of the story:** Always read the label before adopting an \"exotic pet.\" (And maybe donâ€™t trust a guy who carries around an alligator named \"Mr. Bites.\")|"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import asyncio\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AIMessageChunk, HumanMessage\n",
    " \n",
    "# Define the state schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    " \n",
    "# Initialize the LLM with the correct model name and streaming enabled\n",
    "model = ChatMistralAI(\n",
    "    model=\"mistral-medium-latest\",\n",
    "    streaming=True\n",
    ")\n",
    " \n",
    "# Define a node to handle LLM queries\n",
    "async def call_llm(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await model.ainvoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    " \n",
    "# Define the graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_edge(START, \"call_llm\")\n",
    "workflow.add_edge(\"call_llm\", END)\n",
    " \n",
    "app = workflow.compile()\n",
    " \n",
    "# Simulate interaction and stream tokens\n",
    "async def simulate_interaction():\n",
    "    input_message = {\"messages\": [HumanMessage(content=\"Tell me a very long joke\")]}\n",
    "    # Stream LLM tokens\n",
    "    async for msg, metadata in app.astream(input_message, stream_mode=[\"messages\",\"updates\"]):\n",
    "        # Check if we have metadata for call_llm\n",
    "        if isinstance(metadata, dict) and 'call_llm' in metadata:\n",
    "            # Extract the message from call_llm metadata\n",
    "            ai_message = metadata['call_llm']['messages'][0]\n",
    "            if ai_message.content:\n",
    "                print(ai_message.content, end=\"|\", flush=True)\n",
    "if __name__ == \"__main__\":\n",
    "    await simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc53316-e3fa-4afc-bacc-a3b07e477642",
   "metadata": {},
   "source": [
    "### Streaming Custom Data\n",
    "\n",
    "In addition to streaming graph state or tokens, you can also configure nodes to stream custom data that your application might need. The code below simulates producing progress of a long running task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89748f26-33f8-4957-b1da-b3603abc929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'progress': 'Processing step 1/5'}\n",
      "{'progress': 'Processing step 2/5'}\n",
      "{'progress': 'Processing step 3/5'}\n",
      "{'progress': 'Processing step 4/5'}\n",
      "{'progress': 'Processing step 5/5'}\n",
      "{'long_running_node': {'messages': ['Task completed!']}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from time import sleep\n",
    "from langgraph.types import StreamWriter\n",
    "\n",
    "# Define a custom node to simulate a long-running task\n",
    "def long_running_node(state: MessagesState,  writer: StreamWriter):\n",
    "    for i in range(1, 6):\n",
    "        sleep(1)  # Simulate a delay\n",
    "        writer({\"progress\": f\"Processing step {i}/5\"})\n",
    "\n",
    "    return {\"messages\": [\"Task completed!\"]}\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"long_running_node\", long_running_node)\n",
    "workflow.add_edge(START, \"long_running_node\")\n",
    "workflow.add_edge(\"long_running_node\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Simulate interaction and stream custom progress updates\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [(\"human\", \"Start the long-running task\")]}\n",
    "    \n",
    "    for result in app.stream(input_message, stream_mode=[\"custom\",\"updates\"]):\n",
    "        if \"progress\" in result[-1]:\n",
    "            print(result[-1])  # Stream custom progress updates\n",
    "        else:\n",
    "            print(result[-1]) # Stream final message\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d1167-4b91-49a4-8480-c076fb818c21",
   "metadata": {},
   "source": [
    "You can disable streaming for models that do not support streaming. With `streaming=False` argument, streaming is disabled from the model. With streaming disabled, it will output the entire message at once rather than streaming tokens or updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b22b58b-e3e7-467f-841a-1de8e0dd8c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With streaming enabled:\n",
      "To determine how many times the letter \"r\" appears in the word \"strawberry,\" we can break the word down letter by letter. The word \"strawberry\" is spelled as S-T-R-A-W-B-E-R-R-Y. Let's count each occurrence of the letter \"r\" in this sequence.\n",
      "\n",
      "Starting from the beginning, the first \"r\" appears as the third letter in \"strawberry.\" The next \"r\" is the ninth letter, and there is no other \"r\" after that. Therefore, we have two \"r\" sounds in the word. However, it's important to note that the second \"r\" is actually a double \"rr,\" which counts as two letters but is pronounced as a single sound in some dialects. For the purpose of counting individual letters, there are two \"r\"s in \"strawberry.\"\n",
      "\n",
      "In summary, the word \"strawberry\" contains two instances of the letter \"r.\" The first \"r\" is in the middle of the word, and the second \"r\" is part of the double \"rr\" at the end. While the pronunciation may vary, the written form clearly shows two \"r\"s. Thus, the answer is that there are two \"r\"s in \"strawberry.\"\n",
      "\n",
      "With streaming disabled:\n",
      "To determine how many \"r\" letters are in the word \"strawberry,\" let's analyze it step by step. The word \"strawberry\" is spelled as S-T-R-A-W-B-E-R-R-Y. By counting each letter, we can identify the positions of the \"r\" sounds.\n",
      "\n",
      "First, let's break down the word: S, T, R, A, W, B, E, R, R, Y. The \"r\" appears in the third, eighth, and ninth letters of the word. This means there are three instances where the letter \"r\" appears. The first \"r\" is in the middle of the word, while the next two \"r\"s are consecutive, giving the word its distinctive sound.\n",
      "\n",
      "In summary, the word \"strawberry\" contains three \"r\" letters. The repetition of \"r\" in the middle of the word contributes to its phonetic richness, making it a fun and slightly challenging word to pronounce. Whether you're counting letters or practicing pronunciation, \"strawberry\" is a great example of how letters can influence a word's sound and meaning."
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# First example with streaming enabled\n",
    "llm_streaming = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    temperature=1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# Second example with streaming disabled\n",
    "llm_no_streaming = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    temperature=1,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "def create_graph(llm):\n",
    "    graph_builder = StateGraph(MessagesState)\n",
    "    \n",
    "    def chatbot(state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        if not isinstance(messages, list):\n",
    "            messages = [messages]\n",
    "        return {\"messages\": llm.invoke(messages)}\n",
    "\n",
    "    graph_builder.add_node(\"chatbot\", chatbot)\n",
    "    graph_builder.add_edge(START, \"chatbot\")\n",
    "    graph_builder.add_edge(\"chatbot\", END)\n",
    "    return graph_builder.compile()\n",
    "\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"how many r's are in strawberry? Explain in three paragraphs.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"With streaming enabled:\")\n",
    "graph_streaming = create_graph(llm_streaming)\n",
    "for output in graph_streaming.stream(input):\n",
    "    if isinstance(output, dict) and 'chatbot' in output:\n",
    "        # We don't need to print here as StreamingStdOutCallbackHandler handles it\n",
    "        pass\n",
    "\n",
    "print(\"\\n\\nWith streaming disabled:\")\n",
    "graph_no_streaming = create_graph(llm_no_streaming)\n",
    "for output in graph_no_streaming.stream(input):\n",
    "    if isinstance(output, dict) and 'chatbot' in output:\n",
    "        message = output['chatbot']['messages']\n",
    "        print(message.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cda59-2da5-4e38-a078-a6e2fdceebb9",
   "metadata": {},
   "source": [
    "## Considerations for Streaming\n",
    "\n",
    "1. For long tasks or real-time data, streaming provides continuous feedback, reducing the perceived latency for the user.\n",
    "2. Streaming updates rather than full state can reduce overhead and bandwidth when processing large graphs or complex workflows.\n",
    "3. Streaming custom data allows for flexible feedback such as showing progress or intermediate results during execution.\n",
    "4. It's important to handle cases where streaming should be disabled to avoid issues.\n",
    "\n",
    "## External API Integrations\n",
    "\n",
    "API integration allows AI agent to fetch live data, post data such as user information or actions to a web service or interact with third-party tools. In LangGraph, API calls are done inside custom nodes which process the API's response and route the state based on the returned data.\n",
    "\n",
    "1. You need to setup Weather API key from OpenWeatherMap.\n",
    "2. Define a node that sends a request to the open weather map API, parses the response and then returns a message with the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a85a37-38bd-4f94-8347-9f7c178c17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "weather_api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "\n",
    "# Node to fetch live weather data\n",
    "def live_weather_node(state):\n",
    "    city = \"London\"\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={weather_api_key}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        temperature = data['main']['temp']\n",
    "        description = data['weather'][0]['description']\n",
    "        return {\"messages\": [f\"The weather in {city} is {temperature} degree C with {description}.\"]}\n",
    "    else:\n",
    "        return {\"messages\": [\"Sorry, I couldn't fetch the weather information.\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d4f1e-94a7-4dcd-a311-356e3e94f15f",
   "metadata": {},
   "source": [
    "3. Add this node to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0a6577-7239-4084-aa58-c8b93be8bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me the weather in London\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The weather in London is 6.65 degree C with overcast clouds.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"live_weather_node\", live_weather_node)\n",
    "builder.add_edge(START, \"live_weather_node\")\n",
    "builder.add_edge(\"live_weather_node\", END)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [\n",
    "        (\"human\", \"Tell me the weather in London\")\n",
    "    ]}\n",
    "    # Process the input and stream the result\n",
    "    for result in app.stream(input_message, stream_mode=\"values\"):\n",
    "        result['messages'][-1].pretty_print()\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e345d-d89d-497e-95dc-930a27d29e08",
   "metadata": {},
   "source": [
    "You can also extract city information from user input. For this, you need to modify the `live_weather_node` to dynamically fetch the city from user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a78f8b1e-3938-4468-9373-3edbe2e63d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the weather in London?\n",
      "http://api.openweathermap.org/data/2.5/weather?q=london?&appid=a2b6ff65d3e211f7cea9560253f1a015&units=metric\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Sorry, I couldn't fetch the weather information.\n"
     ]
    }
   ],
   "source": [
    "def live_weather_node(state):\n",
    "    last_message = state['messages'][-1].content.lower()\n",
    "    if \"in\" in last_message:\n",
    "        city = last_message.split(\"in\")[-1].strip()\n",
    "    else:\n",
    "        city = \"London\"\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={weather_api_key}&units=metric\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        temperature = data['main']['temp']\n",
    "        description = data['weather'][0]['description']\n",
    "        return {\"messages\": [f\"The weather in {city} is {temperature} degree C with {description}.\"]}\n",
    "    else:\n",
    "        return {\"messages\": [\"Sorry, I couldn't fetch the weather information.\"]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"live_weather_node\", live_weather_node)\n",
    "builder.add_edge(START, \"live_weather_node\")\n",
    "builder.add_edge(\"live_weather_node\", END)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [\n",
    "        (\"human\", \"What's the weather in London?\")\n",
    "    ]}\n",
    "    # Process the input and stream the result\n",
    "    for result in app.stream(input_message, stream_mode=\"values\"):\n",
    "        result['messages'][-1].pretty_print()\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ac1bc-8002-4512-afbc-99256b811f83",
   "metadata": {},
   "source": [
    "Similarly, you can integrate calculator API that can handle arithmetic operations using `mathjs.org`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14671d9a-c38c-445e-b419-9987d122e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Calculate 5 + 3 * 2\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The result of 5 + 3 * 2 is 11.\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "def calculator_node(state):\n",
    "    last_message = state['messages'][-1].content.lower()\n",
    "    expression = last_message.split(\"calculate\")[-1].strip()\n",
    "    # URL encode the expression to ensure it's safe for use in the query string\n",
    "    encoded_expression = urllib.parse.quote(expression)\n",
    "    url = f\"http://api.mathjs.org/v4/?expr={encoded_expression}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        result = response.text\n",
    "        return {\"messages\": [f\"The result of {expression} is {result}.\"]}\n",
    "    else:\n",
    "        return {\"messages\": [\"Sorry, I couldn't calculate that.\"]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"calculator_node\", calculator_node)\n",
    "builder.add_edge(START, \"calculator_node\")\n",
    "builder.add_edge(\"calculator_node\", END)\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    input_message = {\"messages\": [\n",
    "        (\"human\", \"Calculate 5 + 3 * 2\")\n",
    "    ]}\n",
    "    for result in app.stream(input_message, stream_mode=\"values\"):\n",
    "        result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f80579-b732-485e-9534-0be916a82505",
   "metadata": {},
   "source": [
    "You can integrate both APIs using below workflow.\n",
    "1. Create routing function to route requests to either calculator or weather node.\n",
    "2. Build workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "befd09ae-4485-4649-af27-054a11819e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Please calculate 5 * 3 + 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Please calculate 5 * 3 + 2.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The result of 5 * 3 + 2. is 17.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is weather in Texas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is weather in Texas?\n",
      "http://api.openweathermap.org/data/2.5/weather?q=texas?&appid=a2b6ff65d3e211f7cea9560253f1a015&units=metric\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Sorry, I couldn't fetch the weather information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "def routing_function(state):\n",
    "    last_message = state['messages'][-1].content.lower()\n",
    "    if \"weather\" in last_message:\n",
    "        return \"live_weather_node\"\n",
    "    elif \"calculate\" in last_message:\n",
    "        return \"calculator_node\"\n",
    "    return \"default_node\"\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"live_weather_node\", live_weather_node)\n",
    "builder.add_node(\"calculator_node\", calculator_node)\n",
    "builder.add_node(\"default_node\", lambda state: {\"messages\": [\"Sorry, I don't understand the request.\"]})\n",
    "builder.add_conditional_edges(START, routing_function)\n",
    "builder.add_edge(\"live_weather_node\", END)\n",
    "builder.add_edge(\"calculator_node\", END)\n",
    "builder.add_edge(\"default_node\", END)\n",
    "\n",
    "app = builder.compile()\n",
    "\n",
    "def simulate_interaction():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        input_message = {\"messages\": [\n",
    "            (\"human\", user_input)\n",
    "        ]}\n",
    "        for result in app.stream(input_message, stream_mode=\"values\"):\n",
    "            result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "simulate_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b01c3a-ab06-4eff-b688-aac4569112be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
