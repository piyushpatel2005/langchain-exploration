{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a37d34-20f3-4f48-9e60-dc9c7322c81b",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "If you have a use case where you want to ask LLM model questions about topics for which it was not trained, you can use RAG. LLMs always lack certain kinds of information regardless of the amount of web data used to train it.\n",
    "1. Private information: These are information not available to the public.\n",
    "2. Current or Recent events: LLMs are trained on past events or data. They do not have information on the current events.\n",
    "\n",
    "When asked questions about these topics, they will hallucinate in a very convincing fashion.\n",
    "\n",
    "### How to add additional data?\n",
    "\n",
    "To add additional information, you cannot simply include large text into prompt. There are limits to token length as well as it can be very expensive if you send it irrelevant information. So, you can add new information in two steps.\n",
    "1. Index your additional information documents such that LLMs can easily find the most relevant ones for each question.\n",
    "2. Retrieve this data from the index using it as context for LLM to generate better output based on this new dataset.\n",
    "\n",
    "## Why index your documents?\n",
    "\n",
    "If you want to add new information, the information may be in different formats like PDF, image, CSV, JSON, etc. In order to pass this information to LLM, you need to convert them into tokens.\n",
    "\n",
    "1. Extract the text from document\n",
    "2. Split the text into manageable chunks\n",
    "3. Convert text into numbers that computer systems can understand. These numbers are formally called *embeddings*.\n",
    "4. Store these numbers for your text in some store that makes it easy to retrieve the relevant sections of your document to answer a given question. Here, you can use vector database to store embeddings.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "Embeddings was used for full-text search capabilities in websites. In the past, embeddings were based on a sparse matrix which would list if a word occurs in a text or not. That model was useful for keyword search but lacked semantic search because it could not understand the semantic meaning of synonymous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6108b-208d-40d8-80af-0009a2509569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-exploration",
   "language": "python",
   "name": "langchain-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
