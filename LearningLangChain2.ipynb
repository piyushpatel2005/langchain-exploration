{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a37d34-20f3-4f48-9e60-dc9c7322c81b",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "If you have a use case where you want to ask LLM model questions about topics for which it was not trained, you can use RAG. LLMs always lack certain kinds of information regardless of the amount of web data used to train it.\n",
    "1. Private information: These are information not available to the public.\n",
    "2. Current or Recent events: LLMs are trained on past events or data. They do not have information on the current events.\n",
    "\n",
    "When asked questions about these topics, they will hallucinate in a very convincing fashion.\n",
    "\n",
    "### How to add additional data?\n",
    "\n",
    "To add additional information, you cannot simply include large text into prompt. There are limits to token length as well as it can be very expensive if you send it irrelevant information. So, you can add new information in two steps.\n",
    "1. Index your additional information documents such that LLMs can easily find the most relevant ones for each question.\n",
    "2. Retrieve this data from the index using it as context for LLM to generate better output based on this new dataset.\n",
    "\n",
    "## Why index your documents?\n",
    "\n",
    "If you want to add new information, the information may be in different formats like PDF, image, CSV, JSON, etc. In order to pass this information to LLM, you need to convert them into tokens.\n",
    "\n",
    "1. Extract the text from document\n",
    "2. Split the text into manageable chunks\n",
    "3. Convert text into numbers that computer systems can understand. These numbers are formally called *embeddings*.\n",
    "4. Store these numbers for your text in some store that makes it easy to retrieve the relevant sections of your document to answer a given question. Here, you can use vector database to store embeddings.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "Embeddings was used for full-text search capabilities in websites. In the past, embeddings were based on a sparse matrix which would list if a word occurs in a text or not. That model was useful for keyword search but lacked semantic search because it could not understand the semantic meaning of synonymous words.\n",
    "\n",
    "An embedding model is an algorithm that takes text and outputs a numerical representation of its meaning (long list of float numbers about 1000-2000 numbers). These are also called dense embeddings. Different models produce different embeddings. So, it's not possible to use embeddings from one model in another model.\n",
    "\n",
    "One way to claculate the degree of similarity between two vectors is *Cosine similarity*. It computes the dot product of vectors and divides it by the product of their magnitudes to output a number between -1 and 1, where 0 means the vectors share no correlation, -1 means they are dissimilar and 1 means they are absolutely similar. The ability to convert sentences into embeddings that capture semantic meaning and then perform calculations to find semantic similarities between different sentences enables us to get an LLM to find the most relevant documents to answer questions about a large text. There are also models that can produce embeddings for non-textual content such as images, videos and sounds.\n",
    "\n",
    "The embeddings can be used to different applications like search, clustering, classification, recommendation, anomaly detection.\n",
    "\n",
    "### Document to Text conversion\n",
    "\n",
    "The first step of preprocessing is to convert document into text. For this, you need to parse and extract the document with minimal loss of quality. LangChain provides document loaders to handle parsing logic and enable to load data from various sources into a `Document` class that consists of text and associated metadata.\n",
    "\n",
    "Using these loaders follow these steps to parse your data.\n",
    "1. Pick the loader based on the document type.\n",
    "2. Create an instance of loader with parameters to configure it.\n",
    "3. Load the documents by calling `load()` method which returns a list of documetns ready to pass to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c6108b-208d-40d8-80af-0009a2509569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/sample.txt'}, page_content='This is sample text')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./data/sample.txt')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb5ca6-e322-4db6-8e59-d9ed54154df0",
   "metadata": {},
   "source": [
    "LangChain provides document loaders for various formats like CSV, JSON, Markdown. There are loaders to load PDF documents or even `WebBaseLoader` to load HTML documents from URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b47a1b-7bf9-49d3-a9da-2cd9e599067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.langchain.com/\")\n",
    "doc = loader.load()\n",
    "# print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258241b-0fc0-45b2-81c2-6caed9e528a6",
   "metadata": {},
   "source": [
    "You can also load PDF documents using `pypdf` module.\n",
    "\n",
    "```shell\n",
    "uv add pypdf\n",
    "```\n",
    "\n",
    "Then, you can create loader like below.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./test.pdf\")\n",
    "pages = loader.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca8713-d801-40a9-a687-8e49a94dad1d",
   "metadata": {},
   "source": [
    "LLMs and embedding models have hard limit on the size of input and output tokens they can handle. This is called context window. Context window is measured in number of tokens.\n",
    "\n",
    "### Chunking\n",
    "\n",
    "Chunking is spliting text but at the same time keeping semantically related chunks of text together. LangChain provides `RecursiveCharacterTextSplitter` which takes a list of separators which are used to split the text into chunks. These separators are paragraph (`\\n\\n`), line separator (`\\n`) and word separator (` `). This splitter emits each chunk as a `Document` with the metadata of the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f13d0d-c2c7-45ec-8ea3-b23ca2f89c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './data/sample.txt'}, page_content='This is'), Document(metadata={'source': './data/sample.txt'}, page_content='sample'), Document(metadata={'source': './data/sample.txt'}, page_content='text')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"./data/sample.txt\") # or any other loader\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10, # usually set to more than 1000\n",
    "    chunk_overlap=2, # usually about 100 or 200\n",
    ")\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a7524-a8cc-4fcf-9a9e-952706d18fc1",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter` can also be used to split code and makrdown into semantic chunks. This can be done by using keywords specific to each language as the seprators which ensures that the body of each function is kept in the same chunk instead of split between several chunks. LangChain contains separators for various programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a45380-011a-4164-be67-3ea4afa4e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'), Document(metadata={}, page_content='# Call the function\\nhello_world()')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "# Create text splitter for python language\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "print(python_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8333ff-7c8b-43d4-a87a-e0f6bf73c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# LangChain\n",
    "\n",
    "⚡ Building applications with LLMs through composability ⚡\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open source project in a rapidly developing field, we are extremely open \n",
    "    to contributions.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "# you can specify metadata while creating chunked documents\n",
    "md_docs = md_splitter.create_documents([markdown_text], \n",
    "    [{\"source\": \"https://www.langchain.com\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1263531-5a42-4aca-9087-036a5a69dfd0",
   "metadata": {},
   "source": [
    "### Embeddings Generation\n",
    "\n",
    "LangChain also has `Embeddings` class with text embedding models to generate vector representation of text. This class has two methods:\n",
    "1. To embed docuents which takes list of text strings as input.\n",
    "2. To embed a query which takes a single text string.\n",
    "\n",
    "`gemma3:1b` model in Ollama does not support embedding. So, you would need to download [Embedding supported models](https://ollama.com/search?c=embedding).\n",
    "\n",
    "```shell\n",
    "ollama pull embeddinggemma\n",
    "```\n",
    "\n",
    "Also, this lesson covers more powerful model `gemma3:latest`. So, make sure to download it using `ollama pull gemma3:latest` else you might run into errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff66d5ab-773c-47f0-9c00-de0cfe66cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings (\n",
    "    model='embeddinggemma' # gemma3:1b doesn't support embedding\n",
    ")\n",
    "\n",
    "embedding_array = embeddings.embed_documents([\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me world\",\n",
    "    \"Hello World!\"\n",
    "])\n",
    "# print(embedding_array)\n",
    "print(len(embedding_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a3baf-ba15-44d7-a90a-950436e274f1",
   "metadata": {},
   "source": [
    "Full end to end example with OpenAIEmbedding.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "## Load the document \n",
    "\n",
    "loader = TextLoader(\"./test.txt\")\n",
    "doc = loader.load()\n",
    "\n",
    "\"\"\"\n",
    "[\n",
    "    Document(page_content='Document loaders\\n\\nUse document loaders to load data \n",
    "        from a source as `Document`\\'s. A `Document` is a piece of text\\nand \n",
    "        associated metadata. For example, there are document loaders for \n",
    "        loading a simple `.txt` file, for loading the text\\ncontents of any web \n",
    "        page, or even for loading a transcript of a YouTube video.\\n\\nEvery \n",
    "        document loader exposes two methods:\\n1. \"Load\": load documents from \n",
    "        the configured source\\n2. \"Load and split\": load documents from the \n",
    "        configured source and split them using the passed in text \n",
    "        splitter\\n\\nThey optionally implement:\\n\\n3. \"Lazy load\": load \n",
    "        documents into memory lazily\\n', metadata={'source': 'test.txt'})\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "## Split the document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "## Generate embeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "\"\"\"\n",
    "[[0.0053587136790156364,\n",
    " -0.0004999046213924885,\n",
    "  0.038883671164512634,\n",
    " -0.003001077566295862,\n",
    " -0.00900818221271038, ...], ...]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Once the embeddings are created, the next step is to store them in Vector database.\n",
    "\n",
    "## Storing Embeddings\n",
    "\n",
    "A vector store is a database designed to store vectors and perform complex calculations like cosine similarity efficiently and quickly. Vector stores handle unstructured data and are capable of performing create, read, update, dleete and search operations. They can also include scalable applications that utilize AI to answer questions about large documents.\n",
    "\n",
    "There are various vector store providers you can choose from. Some points to consider about vector stores are as below.\n",
    "1. Vector stores are relatively new\n",
    "2. Managing and optimizing vector stores can provide steep learning curve.\n",
    "3. Managing a separate database can add complexity and may drain valuable resources.\n",
    "Vector store capabilities have been extended to PostgreSQL using `pgvector` extension. You can run PostgreSQL from docker using docker compose which will expose postgres instance on port 6024.\n",
    "\n",
    "```shell\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "In order to connect with PostgreSQL, you can use below connection string.\n",
    "\n",
    "```\n",
    "postgresql+psycopg://langchain:langchain@localhost:6024/langchain\n",
    "```\n",
    "\n",
    "In order to use postgres with langchain, you can use `langchain-postgres`.\n",
    "\n",
    "```shell\n",
    "uv add langchain-postgres\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0f8c4d-4fac-473b-99de-cd12c4382025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "# Load the document, split it into chunks\n",
    "raw_documents = TextLoader('./data/sample.txt').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "embeddings_model = OllamaEmbeddings (\n",
    "    model='embeddinggemma'\n",
    ")\n",
    "\n",
    "# save embeddings into vector database\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "db = PGVector.from_documents(documents, embeddings_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498f7df0-c84c-465a-b7fd-6314408ef39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='240ee268-50e7-4f64-8f27-80035ddab5c2', metadata={'source': './data/sample.txt'}, page_content='This is sample text'),\n",
       " Document(id='b84aada4-ae5c-42f9-ac8e-b51ec59e94c8', metadata={'source': './data/sample.txt'}, page_content='This is sample text'),\n",
       " Document(id='6b064060-a286-45ed-8859-ebe310cdc62b', metadata={'source': './data/sample.txt'}, page_content='This is sample text'),\n",
       " Document(id='d1f1aaca-16dc-41a2-9f10-aa5115e048ed', metadata={'source': './data/sample.txt'}, page_content='This is sample text')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"query\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24966db1-c8a9-494b-a51f-3ceb512c1d30",
   "metadata": {},
   "source": [
    "Above query will find `k=4` matching embeddings that are similar to your query. In this case, the word `query` is sent to embedding model to retrieve its similar documents.\n",
    "\n",
    "You can add more documents to the database using `add_documents` method ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866405e7-aeda-4df9-8363-cdac22aa3c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['96fcb891-93ee-4263-a468-ddf949b81ffe',\n",
       " '071aeb30-99c8-4a92-bc2c-311744849607']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content=\"there are cats in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"ducks are also found in the pond\",\n",
    "            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\n",
    "        ),\n",
    "    ],\n",
    "    ids=ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a28ea4-e2f5-4673-a314-f808d4401e33",
   "metadata": {},
   "source": [
    "If you need to delete an entry, you can simplly run `db.delete(ids=[1])` to delete entry by ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e9cb5-ae6e-43c1-bb08-32c968b28a79",
   "metadata": {},
   "source": [
    "### Tracking Document Changes\n",
    "\n",
    "When documents change, you need to re-index the document. This can be costly as embeddings need to be recomputed. LangChain provides indexing API to make it easy to keep your documents in sync with your vector store. The API uses a class `RecordManager` to keep track of document writes into the vector store. When indexing content, hashes are computed for each document and the following information is stored in `RecordManager`.\n",
    "- the document hash (including content and metadata)\n",
    "- write time\n",
    "- source ID\n",
    "you can also provide cleanup modes to decide how to delete existing documents in the store. If source documents have changed, you may want to remove any existing documents that come from the same source as the new documents being indexed. The modes are as follows:\n",
    "- `None`: This does not do any automatic cleanup and user will have to do manual cleanup\n",
    "- `Incremental`: This will delete previous versions of the content if the content of the source document or derived documents has changed.\n",
    "- `Full`: This will do same as `Incremental` but also delete any documents not included in documents currently being indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c2a701-8902-49d1-8cb4-14cb85d25676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushpatel/IdeaProjects/langchain-exploration/.venv/lib/python3.9/site-packages/langchain_core/indexing/api.py:389: UserWarning: Using SHA-1 for document hashing. SHA-1 is *not* collision-resistant; a motivated attacker can construct distinct inputs that map to the same fingerprint. If this matters in your threat model, switch to a stronger algorithm such as 'blake2b', 'sha256', or 'sha512' by specifying  `key_encoder` parameter in the `index` or `aindex` function. \n",
      "  _warn_about_sha1()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 1: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n",
      "Index attempt 2: {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n",
      "Index attempt 3: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\t\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"my_docs\"\n",
    "embeddings_model = OllamaEmbeddings (\n",
    "    model='embeddinggemma'\n",
    ")\n",
    "namespace = \"my_docs_namespace\"\n",
    "\t\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\t\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,\n",
    "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n",
    ")\n",
    "\t\n",
    "# Create the schema if it doesn't exist\n",
    "record_manager.create_schema()\n",
    "\t\n",
    "# Create documents\n",
    "docs = [\n",
    "    Document(page_content='there are cats in the pond', metadata={\n",
    "        \"id\": 1, \"source\": \"cats.txt\"}),\n",
    "    Document(page_content='ducks are also found in the pond', metadata={\n",
    "        \"id\": 2, \"source\": \"ducks.txt\"}),\n",
    "]\n",
    "\t\n",
    "# Index the documents\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",  # prevent duplicate documents\n",
    "    source_id_key=\"source\",  # use the source field as the source_id\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 1:\", index_1)\n",
    "\t\n",
    "# second time you attempt to index, it will not add the documents again\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 2:\", index_2)\n",
    "\t\n",
    "# If we mutate a document, the new version will be written and all old \n",
    "# versions sharing the same source will be deleted.\n",
    "\t\n",
    "docs[0].page_content = \"I just modified this document!\"\n",
    "\t\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 3:\", index_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6b07b-c37d-4007-8f80-c40c3cf1273f",
   "metadata": {},
   "source": [
    "### Indexing Optimization\n",
    "\n",
    "There are various strategies to enhance the accuracy and performance of the indexing stage.\n",
    "\n",
    "#### MultiVectorRetriever\n",
    "\n",
    "A document containing mix of text and tables cannot be simply split by text into chunks or embedded as context: the table could be lost. To solve this, you could decouple documents that you want to use for answer synthesis. For a document containing tables, we can generate and embed summaries of table elements with each summary containing the `id` reference to the full raw table. Next, store the raw referenced tables in a separate doc store. When user query retrieves a table summary, you also pass the entire referenced raw table as context to the final prompt sent to the LLM for answer synthesis. This way you provide the model with the full context of information required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99dbd901-000a-4876-9d09-37125e6725bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of loaded docs:  19\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid\n",
    "\t\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"summaries\"\n",
    "embeddings_model = OllamaEmbeddings(model='embeddinggemma')\n",
    "# Load the document\n",
    "loader = TextLoader(\"./data/sample.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\t\n",
    "print(\"length of loaded docs: \", len(docs[0].page_content))\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\t\n",
    "# The rest of your code remains the same, starting from:\n",
    "prompt_text = \"Summarize the following document:\\n\\n{doc}\"\n",
    "\t\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "llm = ChatOllama(temperature=0, model=\"gemma3:latest\")\n",
    "summarize_chain = {\n",
    "    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\t\n",
    "# batch the chain across the chunks\n",
    "summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fffca1e-2366-4e39-bca0-07f6dba84bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\t\n",
    "# indexing the summaries in our vector store, whilst retaining the original \n",
    "# documents in our document store:\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\t\n",
    "# Changed from summaries to chunks since we need same length as docs\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\t\n",
    "# Each summary is linked to the original document by the doc_id\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\t\n",
    "# Add the document summaries to the vector store for similarity search\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\t\n",
    "# Store the original documents in the document store, linked to their summaries \n",
    "# via doc_ids\n",
    "# This allows us to first search summaries efficiently, then fetch the full \n",
    "# docs when needed\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\t\n",
    "# vector store retrieves the summaries\n",
    "sub_docs = retriever.vectorstore.similarity_search(\n",
    "    \"chapter on philosophy\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811b6de7-591b-45b5-a441-dbf61a281565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whereas the retriever will return the larger source document chunks:\n",
    "retrieved_docs = retriever.invoke(\"chapter on philosophy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99b33ac4-166c-4922-b4f5-6f1254c63af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './data/sample.txt'}, page_content='This is sample text')]\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624d75e-3357-4257-9d1b-2e13f13988bf",
   "metadata": {},
   "source": [
    "#### RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
    "\n",
    "RAG need to handle lower-level questions regarding specific facts found in a single document or higher-level questions that distill ideas that span many documents. This can be challenging with typical k-nearest neighbors retrieval over document chunks.\n",
    "\n",
    "RAPTOR involves creating document summaries that capture higher-level concepts, embeddings and clustering those ducments and then summarizing each cluster. This is done recursively to create tree of summaries with increasingly high-level concepts. The summaries and initial documents are then indexed together, giving coverage across lower-to-higher-level user questions.\n",
    "\n",
    "#### ColBERT: Optimizing Embeddings\n",
    "\n",
    "During indexing stage, the embedding models compress text into fixed-length vector representation taht captures the semantic content of the document. This compression may lead to hallucinations in the final LLM output. You can do following.\n",
    "1. Generate contextual embeddings for each token in the document and query.\n",
    "2. Calculate and score similarity between each query token and all document tokens.\n",
    "3. Sum the maximum similarity score of each query embedding to any of the document embeddings to get a score for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7426ef-ff09-4831-979e-554e1febc612",
   "metadata": {},
   "source": [
    "## Retrieve Embeddings and Documents\n",
    "\n",
    "The process of embedding user's query, retrieving similar documents from a data source and then passing them as context to the prompt sent to the LLM is known as **retrieval augmented generation (RAG)**. This is essential component of building chat-enabled LLM apps which are accurate, efficient and up to date.\n",
    "\n",
    "With RAG, LLM only relies on pre-trained data which is usually outdated in few days or months. RAG systems follow below stages:\n",
    "1. Indexing: This is where we input new data source and store embeddings that will be stored in vector store. This involves various substages like loading, text splitting (chunking), embedding and storing into vector store.\n",
    "2. Retrieval: This stage involves retrieving the relevant embeddings and the data stored in the vector store based on user's query.\n",
    "3. Generation: This involves synthesizing the original prompt with the retrieved relevant documents as final prompt sent to the model for prediction.\n",
    "\n",
    "In order to perform retrieval, you need to perform similarity search calculations between user's query and stored embeddings so that relevant chunks of documents are retrieved. This includes following steps.\n",
    "1. Convert user's query into embeddings\n",
    "2. Calculate the embeddings in the vector store that are most similar to user's query.\n",
    "3. Retrieve the relevant document embeddings and their text chunks.\n",
    "\n",
    "This is done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845f34da-d163-4be1-a08f-0b515e62b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='240ee268-50e7-4f64-8f27-80035ddab5c2', metadata={'source': './data/sample.txt'}, page_content='This is sample text'), Document(id='b84aada4-ae5c-42f9-ac8e-b51ec59e94c8', metadata={'source': './data/sample.txt'}, page_content='This is sample text'), Document(id='6b064060-a286-45ed-8859-ebe310cdc62b', metadata={'source': './data/sample.txt'}, page_content='This is sample text'), Document(id='d1f1aaca-16dc-41a2-9f10-aa5115e048ed', metadata={'source': './data/sample.txt'}, page_content='This is sample text')]\n"
     ]
    }
   ],
   "source": [
    "# create retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# fetch relevant docs\n",
    "docs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history of philosophy?\"\"\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87874d6-01cf-46fa-959f-ebf591336087",
   "metadata": {},
   "source": [
    "The function `as_retriever` does the heavy lifting and abstracts the logic of embedding the user's query and the underlying similarity search calculations performed by the vector store to retrieve the relevant docs.\n",
    "\n",
    "You can also specify the number of documents to retrive as parameter `k`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7190b8-adbe-4e56-80b0-e36decd69902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# create retriever with k=2\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# fetch the 2 most relevant documents\n",
    "docs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history of philosophy?\"\"\")\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ba66e-04d2-4176-b5d9-505a54757564",
   "metadata": {},
   "source": [
    "The more documents you retrieve, the slower your application will perform, the larger the prompt will be and the greater the likelihood of retrieving chunks of text that are irrelevant which can cause the LLM to hallucinate.\n",
    "\n",
    "## Generating LLM Predictions\n",
    "\n",
    "Once docs have been retrieved based on user's query, you add them to the original prompt as context and then invoke the model to get the final output. You can do it using below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f81ff956-7856-48fc-ac74-fadadd403ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/6k7kx0813nj31c19df8s6xs40000gp/T/ipykernel_33903/2636585734.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"\"\"Who are the key figures in the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The provided text does not contain information about key figures in ancient Greek history of philosophy. It only contains repeated sample text.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-19T01:53:26.979607Z', 'done': True, 'done_reason': 'stop', 'total_duration': 792350000, 'load_duration': 132388875, 'prompt_eval_count': 270, 'prompt_eval_duration': 312856958, 'eval_count': 25, 'eval_duration': 335919041, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--a5e0e8e5-94bd-4cd3-9ffd-4abc2ef18e1e-0', usage_metadata={'input_tokens': 270, 'output_tokens': 25, 'total_tokens': 295})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \n",
    "    the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# fetch relevant documents \n",
    "docs = retriever.get_relevant_documents(\"\"\"Who are the key figures in the \n",
    "    ancient greek history of philosophy?\"\"\")\n",
    "\n",
    "# run\n",
    "chain.invoke({\"context\": docs,\"question\": \"\"\"Who are the key figures in the \n",
    "    ancient greek history of philosophy?\"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff073d-1713-4ef2-88ca-23c7398f707a",
   "metadata": {},
   "source": [
    "You can encapsulate most of these code in a single function while passing user's question as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83302705-6774-416c-83e9-c6287ef6429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The provided text does not contain information about key figures in ancient Greek history of philosophy. It only contains sample text repeated across four documents.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-19T01:53:29.579142Z', 'done': True, 'done_reason': 'stop', 'total_duration': 685024625, 'load_duration': 130796000, 'prompt_eval_count': 267, 'prompt_eval_duration': 158029958, 'eval_count': 28, 'eval_duration': 382923626, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--ca963da1-3564-4936-822b-9c4b1b5fb7a7-0', usage_metadata={'input_tokens': 267, 'output_tokens': 28, 'total_tokens': 295})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \n",
    "    the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatOllama(model='gemma3:latest', temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents \n",
    "    docs = retriever.get_relevant_documents(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "qa.invoke(\"Who are the key figures in the ancient greek history of philosophy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b2ab73-fc60-48b2-b27c-30ecd3d64790",
   "metadata": {},
   "source": [
    "In above code, `@chain` decorator turns the function into a runnable chain.\n",
    "You could also return the retrieved documents for further inspection like model performance and evaluations.\n",
    "\n",
    "```python\n",
    "@chain\n",
    "def qa(input):\n",
    "    ... ...\n",
    "    return {\"answer\": answer, \"docs\": docs}\n",
    "```\n",
    "\n",
    "There are some questions to answer for production grade AI RAG apps.\n",
    "- How to handle variability in the quality of a user's input?\n",
    "- How do you transform the natural language to the query language of the target data source?\n",
    "- How to optimize indexing process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cfe06-fc41-488d-9669-5cf33bf5627f",
   "metadata": {},
   "source": [
    "## Query Transformation\n",
    "\n",
    "It is one of the strategies to modify the user's input to answer the first RAG problems. This can help make user's input more or less abstract in order to generate an accurate LLM output.\n",
    "\n",
    "### Rewrite-Retrieve-Read\n",
    "\n",
    "This strategy simply prompts LLM to rewrite the user's query before performing retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bca2b21-a4a7-43bd-b080-2ac4fb3b217d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The provided documents contain only sample text and do not contain any information about ancient Greek philosophy or key figures.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-19T02:01:15.209828Z', 'done': True, 'done_reason': 'stop', 'total_duration': 546278625, 'load_duration': 131671042, 'prompt_eval_count': 295, 'prompt_eval_duration': 103272417, 'eval_count': 22, 'eval_duration': 300879124, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--735f26f6-44b0-4e7f-a685-75a1e4030ac7-0', usage_metadata={'input_tokens': 295, 'output_tokens': 22, 'total_tokens': 317})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@chain\n",
    "def qa(input):\n",
    "    # fetch relevant documents \n",
    "    docs = retriever.get_relevant_documents(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "qa.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient greek history of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14e6a6-54d9-4fa2-a2df-ae81ad3f1987",
   "metadata": {},
   "source": [
    "In above case, model fails to answer the question because it was distracted by the irrelevant information in user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fca71f98-6e4f-4af8-85ff-ca283f5146c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The provided documents contain only sample text and do not contain any information about ancient Greek philosophy or key figures.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-19T02:15:16.191295Z', 'done': True, 'done_reason': 'stop', 'total_duration': 705702500, 'load_duration': 133501417, 'prompt_eval_count': 295, 'prompt_eval_duration': 259902000, 'eval_count': 22, 'eval_duration': 301030044, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--2fde0d75-5da5-4c44-bee7-2b72c2202111-0', usage_metadata={'input_tokens': 295, 'output_tokens': 22, 'total_tokens': 317})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search query for web search engine to answer the given question, end the queries with ’**’. Question: {x} Answer:\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\"').strip(\"**\")\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "    new_query = rewriter.invoke(input)\n",
    "    # fetch relevant documents \n",
    "    docs = retriever.get_relevant_documents(new_query)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "# run\n",
    "qa_rrr.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient greek history of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f7299-e484-4a1e-9c3a-abb6999c027a",
   "metadata": {},
   "source": [
    "In this case, our model is strong enough to understand the question, but if the model was larger size, it would output correct result for the question.\n",
    "\n",
    "The downside of this approach is that it will introduce additional latency as LLM has to first rewrite the question and then pass it over to get the answer.\n",
    "\n",
    "### Multi-Query Retrieval\n",
    "\n",
    "A user's single query can be insufficient to capture the full scope of information required to answer the query completely. This approach resolves this problem by instructing an LLM to generate multiple queries based on a user's initial query, executing a parallel retrieval of each query from the data source and then inserting the retrieved results as prompt context to generate a final model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29c18eb9-62a1-417f-9e56-0d4f7e00ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an AI language \\n    model assistant. Your task is to generate five different versions of the \\n    given user question to retrieve relevant documents from a vector database. \\n    By generating multiple perspectives on the user question, your goal is to \\n    help the user overcome some of the limitations of the distance-based \\n    similarity search. Provide these alternative questions separated by \\n    newlines. Original question: {question}'), additional_kwargs={})]) middle=[ChatOllama(model='gemma3:latest', temperature=0.0)] last=RunnableLambda(parse_queries_output)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language \n",
    "    model assistant. Your task is to generate five different versions of the \n",
    "    given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to \n",
    "    help the user overcome some of the limitations of the distance-based \n",
    "    similarity search. Provide these alternative questions separated by \n",
    "    newlines. Original question: {question}\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "print(query_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba240e60-a4e8-409d-be87-e8a4a04eea44",
   "metadata": {},
   "source": [
    "Once you've received generated queries, you can retrieve the most relevant docs for each of them in parallel and then combine them to get the unique union of all the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fad2331-244f-42b8-8fc9-b462a0b7b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(document_lists):\n",
    "    # Flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc\n",
    "        for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7aab50-d377-472c-9813-e744f11d9587",
   "metadata": {},
   "source": [
    "Some of the generated questions might repeat the same documents so we may need to deduplicate them. In above code, `retriever.batch` runs all generated queries in parallel and returns a list of results which is deduped.\n",
    "\n",
    "The last step is to contruct the prompt which includes user's question and combined retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3353e9fc-a887-43ed-9fa7-0fe95a9c15fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This document provides no information about key figures in ancient Greek history of philosophy. It simply states, \"This is sample text.\" \\n\\nTo answer your question, you would need a different document containing that information.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-19T03:07:55.830937Z', 'done': True, 'done_reason': 'stop', 'total_duration': 832136625, 'load_duration': 134286458, 'prompt_eval_count': 99, 'prompt_eval_duration': 95054667, 'eval_count': 43, 'eval_duration': 583231377, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--324a14dd-3679-4258-9859-c5176a6e0890-0', usage_metadata={'input_tokens': 99, 'output_tokens': 43, 'total_tokens': 142})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n",
    "    on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents \n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "# run\n",
    "multi_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \n",
    "    of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375cadd-d426-4af7-83d4-b817fe5a081c",
   "metadata": {},
   "source": [
    "The unique multi-query retrieval is contained in `retrieval_chain`. This is key idea of making each technique as a standalone chain so that you can easily adopt and combined them as you like.\n",
    "\n",
    "### RAG-Fusion\n",
    "\n",
    "This strategy is similar to multi-query retrieval except you will apply a final reranking step to all retrieved documents. This step makes use of reciprocal rank fusion (RRF) algorithm which produces a single unified ranking. RRF is well-suited for combining results from queries that might have different scales or distribution of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f95066d-7db0-44fc-b406-7a2c1aa11311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful \n",
    "    assistant that generates multiple search queries based on a single input \n",
    "    query. \\n\n",
    "    Generate multiple search queries related to: {question} \\n\n",
    "    Output (4 queries):\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "llm = ChatOllama(model='gemma3:latest', temperature=0)\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d8b75-119e-45d0-aede-0b058aeba8ec",
   "metadata": {},
   "source": [
    "Above code generates queries and then parses the queries as separate questions list. \n",
    "\n",
    "The `reciprocal_rank_fusion` function takes a list of search results of each query (list of list of documents) where inner list of documents is sorted by their relevance to each query. The RRF algorithm then calculates a new score for each document based on its ranks in the different lists and sorts them to create a final re-ranked list. After calculating scores, the function sorts the documents in descending order of their scores to get final re-ranked list. This is returned as the result of this function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "850b5877-b273-4ec9-85c7-17850b367b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    reciprocal rank fusion on multiple lists of ranked documents \n",
    "       and an optional parameter k used in the RRF formula\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each document\n",
    "    # Documents will be keyed by their contents to ensure uniqueness\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list,\n",
    "        # with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Use the document contents as the key for uniqueness\n",
    "            doc_str = doc.page_content\n",
    "            # If the document hasn't been seen yet,\n",
    "            # - initialize score to 0\n",
    "            # - save it for later\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            # Update the score of the document using the RRF formula:\n",
    "            # 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order \n",
    "    # to get the final reranked results\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True\n",
    "    )\n",
    "    # retrieve the corresponding doc for each doc_str\n",
    "    return [\n",
    "        documents[doc_str]\n",
    "        for doc_str in reranked_doc_strs\n",
    "    ]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e674bcf-3133-4227-bdcf-033e48c4439e",
   "metadata": {},
   "source": [
    "The parameter `k` in `reciprocal_rank_fusion` function determines how much influence documents in each query's results have over the final list of documents. A higher value means lower-ranked documents have more influence. Finally, you can combine retrieval chain with full chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7be64a3d-5f9e-45a9-9241-13ed71074915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This document provides no information about key figures in ancient Greek history of philosophy. It simply states, \"This is sample text.\" \\n\\nTo answer your question, you would need a different document containing that information.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-20T05:25:45.188753Z', 'done': True, 'done_reason': 'stop', 'total_duration': 860235250, 'load_duration': 132388208, 'prompt_eval_count': 101, 'prompt_eval_duration': 123557042, 'eval_count': 43, 'eval_duration': 583062918, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--402f592a-1004-4914-b5eb-a92a09e3fa36-0', usage_metadata={'input_tokens': 101, 'output_tokens': 43, 'total_tokens': 144})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n",
    "    on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(model='gemma3:latest', temperature=0)\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents \n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "multi_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \n",
    "    of philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e514bd-e974-489c-986d-9a23dbb5a2b2",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "This is a strategy which involves creating a hypotheitcal document based on user's query, embedding the document and retrieving relevant documents based on vector similarity. The intuition behind HyDE is that LLM-generated hypothetical document will be more similar to the most relevant documents than original query.\n",
    "\n",
    "1. Define prompt to generate hypothetical document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f71bfb8-5219-4f11-b6eb-38ecd61f3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to \n",
    "   answer the question.\\n Question: {question} \\n Passage:\"\"\")\n",
    "\n",
    "generate_doc = (\n",
    "    prompt_hyde | ChatOllama(model='gemma3:latest', temperature=0) | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69772f-59e3-41e0-96cb-b5b4460dd65d",
   "metadata": {},
   "source": [
    "2. Take the hypothetical document and use it as input to the retriever which generates embedding and search for similar documents in vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1775133-a029-4571-839c-9a5b76414c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = generate_doc | retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db0a00-bcae-464c-9b1a-dd5986c88008",
   "metadata": {},
   "source": [
    "3. Take the retrieved documents, pass as context to the final prompt and instruct model to generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d79361ac-fa75-48c1-9d03-5e0b0799517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This document does not contain information about key figures in ancient Greek history of philosophy. It only contains sample text from four documents all with the same content: \"This is sample text\". \\n\\nTo answer your question, you would need a document that discusses Greek philosophers like Plato, Aristotle, Socrates, etc.', additional_kwargs={}, response_metadata={'model': 'gemma3:latest', 'created_at': '2025-12-20T05:32:30.343002Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1267862500, 'load_duration': 143974791, 'prompt_eval_count': 270, 'prompt_eval_duration': 236376417, 'eval_count': 62, 'eval_duration': 858898751, 'logprobs': None, 'model_name': 'gemma3:latest'}, id='run--f53578f2-cea4-4c21-8d6f-b85fd1f52989-0', usage_metadata={'input_tokens': 270, 'output_tokens': 62, 'total_tokens': 332})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \n",
    "    on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(model='gemma3:latest', temperature=0)\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "  # fetch relevant documents from the hyde retrieval chain defined earlier\n",
    "  docs = retrieval_chain.invoke(input)\n",
    "  # format prompt\n",
    "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "  # generate answer\n",
    "  answer = llm.invoke(formatted)\n",
    "  return answer\n",
    "\n",
    "qa.invoke(\"\"\"Who are some key figures in the ancient greek history of \n",
    "    philosophy?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca839c-0eaa-4bc6-b3b0-a6510b7a3cc3",
   "metadata": {},
   "source": [
    "## Query Routing\n",
    "\n",
    "The required data may live in a variety of data sources including RDBMs or other vector databases. You may need to route the query to the appropriate inferred data source to retrieve relevant docs. Query routing is used to forward a user query to relevant data source.\n",
    "\n",
    "### 1. Logical Routing\n",
    "\n",
    "In this, you give LLM knowledge of various data sources at your disposal and then let LLM reason which data source to apply based on user query. You can use function-calling models to help classify each query into one of the available routes. A function call involves defining a schema taht the model can use to generate arguments of a function based on the query. Below code identifies which retriever to call: Python based on JS based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39130d04-6757-40e8-a629-e5b09890eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Given a user question, choose which datasource would be \n",
    "            most relevant for answering their question\"\"\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data \n",
    "    source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the \n",
    "    relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcef6e-9692-4da3-9421-1d1225e9ee05",
   "metadata": {},
   "source": [
    "Next, you can invoke LLM to extract the data source based on predefined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36527e7b-f439-4d5f-93bf-074e7e6a8e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result.datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787151ba-3766-427c-9934-f71552351a32",
   "metadata": {},
   "source": [
    "LLM produced output with `datasource` key based on the schema defined in the `RouteQuery`. Once you've extracted the relevant data source, you can pass the value into another function to execute extra logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10c1519d-0787-4721-9b54-d349491e2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def choose_route(result):\n",
    "    # don't match the string exactly to make it more resilient\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23774e1-a627-4ff7-be01-71a5c87e38c0",
   "metadata": {},
   "source": [
    "Logical routing is suitable when you have a defined list of data sources from which relevant data can be retrieved and utilized by LLM to generate output.\n",
    "\n",
    "### 2. Semantic Routing\n",
    "\n",
    "This involves embedding various prompts that represent various data sources alongside the user query and then performing vector similarity search to retrieve similar prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3915d1c2-7a87-4ff3-90f4-5755002ab167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right then, hello there! I’m Albert, and I specialize in unraveling the mysteries of the universe. \n",
      "\n",
      "Now, let’s talk about black holes. Simply put, a black hole is a region in spacetime where gravity is so incredibly strong that *nothing*, not even light, can escape. \n",
      "\n",
      "Here’s the breakdown:\n",
      "\n",
      "*   **Formation:** They typically form when massive stars run out of fuel and collapse under their own gravity.\n",
      "*   **Singularity:** At the center of a black hole is a point called a singularity – a place where all the star's mass is crushed into an infinitely small space.\n",
      "*   **Event Horizon:**  Surrounding the singularity is the event horizon – it’s the “point of no return.” Once something crosses this boundary, it’s pulled into the black hole and can never escape.\n",
      "\n",
      "It’s a fascinating and frankly, quite bizarre concept! Do you want me to delve into any particular aspect of black holes, like how they affect spacetime or how we detect them?\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor name Albert. You are great at \n",
    "    answering questions about physics in a concise and easy-to-understand manner. \n",
    "    When you don't know the answer to a question, you admit that you don't know. \n",
    "    Also, before answering question, always introduce yourself in one line.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician named John. You are great at answering \n",
    "    math questions. You are so good because you are able to break down hard \n",
    "    problems into their component parts, answer the component parts, and then \n",
    "    put them together to answer the broader question. \n",
    "    Also, before answering question, introduce yourself in one line.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = OllamaEmbeddings(model='embeddinggemma')\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    # Pick the prompt most similar to the input question\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "semantic_router = (\n",
    "    prompt_router\n",
    "    # | ChatOpenAI()\n",
    "    | ChatOllama(model='gemma3:latest', temperature=0.05)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(semantic_router.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a70f0-bc0c-4532-9c87-549ded27ea54",
   "metadata": {},
   "source": [
    "This is how you can route user's query to relevant data source.\n",
    "\n",
    "## Query Construction\n",
    "\n",
    "Query construction is the process of transforming a natural language query into the query language of the database or data source you are interacting with.\n",
    "\n",
    "### 1. Text-to-Metadata Filter\n",
    "\n",
    "Most vector stores provide the ability to limit vector search based on metadata. During embedding, you can attach metadata key-value pairs to vectors in an index and then specify filter expressions when you query the index. LangChain provides `SelfQueryRetriever` that provides this logic and makes it easier to translate natural language queries into structured queries for various data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31f1c39a-bba9-4ebd-876f-bd2d756992a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "description = \"Brief summary of a movie\"\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatOllama (model='gemma3:latest', temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, db, description, fields,\n",
    ")\n",
    "\n",
    "print(retriever.invoke(\n",
    "    \"What's a highly rated (above 8.5) science fiction film?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bb775-2160-4bed-9085-a5597942c7eb",
   "metadata": {},
   "source": [
    "Retriever will take in user query and split it into:\n",
    "1. A filter to apply on metadata of each document first\n",
    "2. A query to use for semantic search on the documents\n",
    "\n",
    "The retriver will\n",
    "1. send query generation prompt to LLM\n",
    "2. Parse metadata filter and rewritten search query from the LLM output\n",
    "3. Convert metadata filter generated by LLM to appropriate format for vector store.\n",
    "4. Issue a similarity search against vector store, filtered to match documents whose metadata passes the generated filter.\n",
    "\n",
    "### 2. Text to SQL\n",
    "\n",
    "We can use LLM to translate user query to SQL queries. For this to work effectively, you can use following strategies.\n",
    "1. For LLM to generate proper SQL queries, it needs accurate description of the database. You could provide LLM with a `CREATE TABLE` description of each table with column information. You could also provide few example records.\n",
    "2. Providing the prompt with few-shot examples of question-query matches can improve the query generation accuracy.\n",
    "\n",
    "```python\n",
    "from langchain_community.tools import QuerySQLDatabaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# replace this with the connection details of your db\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "# llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "llm = ChatOllama(model=\"gemma3:latest\", temperature=0)\n",
    "\n",
    "# convert question to sql query\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "# Execute SQL query\n",
    "execute_query = QuerySQLDatabaseTool(db=db)\n",
    "\n",
    "# combined\n",
    "chain = write_query | execute_query\n",
    "\n",
    "# invoke the chain\n",
    "chain.invoke('How many employees are there?');\n",
    "```\n",
    "\n",
    "First, you convert user query to SQL query appropriate to the dialect of the database. Then, you execute the query on database. This can be risky on database. So, always make sure that you run the queries with a user with readonly access. Add a timeout to queries run by this application to ensure that even if an expensive query is generated, it is cancelled before taking up too many resources. Also, restrict access to only required tables for this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a57aa7-26fb-4c59-8fae-21897f8a9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-exploration",
   "language": "python",
   "name": "langchain-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
