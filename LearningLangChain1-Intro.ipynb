{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dec774a-69b2-4a82-86c6-3474a68d7e8d",
   "metadata": {},
   "source": [
    "# LLM Foundations and LangChain\n",
    "\n",
    "## Set up Ollama\n",
    "\n",
    "To play with LLMs, you can use Ollama open source models. You can download them and play with those models in your local laptop.\n",
    "\n",
    "```shell\n",
    "brew install ollama\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "## Setting up connection\n",
    "\n",
    "In order to interact with Langchain, you can follow three steps.\n",
    "1. You can just import respective library from langchain. For example, to interact with Ollama models, you can install `langchain-ollama` package and then import respective class.\n",
    "2. Create model instance for the LLM model you want to work with\n",
    "3. Invoke the model with a prompt using `invoke()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf75e4f0-13d0-4c39-b035-d632b6bb2105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is **blue**. \\n\\nBut it can be many other colors too! ðŸ˜Š \\n\\nDo you want to tell me why itâ€™s blue? Or would you like to talk about something else related to the sky?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# If you increase temperature, it becomes more creative and may hallucinate\n",
    "model = OllamaLLM(model=\"gemma3:1b\", temperature=0.1, max_tokens=1)\n",
    "\n",
    "model.invoke(\"The sky is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17967187-1bb3-49f5-bf0a-f4fed9ef309e",
   "metadata": {},
   "source": [
    "The `OllamaLLM` accepts below parameters.\n",
    "- `model`: This is very common parameter to configure and specifies the model to use. Most providers have multiple models.\n",
    "- `temperature`: This controls sampling algorithm used to generate output. Lower values produce more predictable outputs. For example, creative writing might need higher values for temperature.\n",
    "- `max_tokens`: This limits the size of the output. Sometimes it may truncate output if this value is set to very low value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fd572-6661-4e3a-b215-f0e264503b90",
   "metadata": {},
   "source": [
    "The chat models enable back and forth conversations. This will have different messsages: user, assistant and system roles.\n",
    "- System role: Used to provide instructions to the model about how to answer a user question.\n",
    "- User role: Used for the user's query\n",
    "- Assistant role: Used for content generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc6c454-3df6-44c1-bb9c-7a8c9f8d7735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris. ðŸ˜Š \\n\\nWould you like to know more about Paris?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eedc84-4230-4f2f-9199-a28c327c78aa",
   "metadata": {},
   "source": [
    "There are different types of messages.\n",
    "\n",
    "- `SystemMessage`: For setting the instructions the AI should follow with system role.\n",
    "- `HumanMessage`: A message sent from the human with the user role.\n",
    "- `AIMessage`: A message sent from teh LLM with the assistant role.\n",
    "- `ChatMessage`: A message with arbitrary setting of role.\n",
    "\n",
    "Let's include `SystemMessage` to interact with model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34775955-6a3d-42aa-9aa7-562be479c565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris! ðŸŽ‰ðŸ‡«ðŸ‡·', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T18:27:14.952783Z', 'done': True, 'done_reason': 'stop', 'total_duration': 880471125, 'load_duration': 685428083, 'prompt_eval_count': 36, 'prompt_eval_duration': 76550083, 'eval_count': 11, 'eval_duration': 90621669, 'logprobs': None, 'model_name': 'gemma3:1b'}, id='run--5aafb45a-661d-4037-af38-e6121641fd72-0', usage_metadata={'input_tokens': 36, 'output_tokens': 11, 'total_tokens': 47})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0\n",
    ")\n",
    "system_message = SystemMessage(\n",
    "    '''You are a very helpful assistant that responds to questions with three exclamation marks.'''\n",
    ")\n",
    "human_message = HumanMessage('What is the capital of France?')\n",
    "\n",
    "model.invoke([system_message, human_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56ca3e-d821-4275-b99b-413481dc5de5",
   "metadata": {},
   "source": [
    "To send these interactive conversations programmatically, you can use LangChain's `PromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b627e032-535d-4577-9296-3512e3272b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the\\n    context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
    "    context below. If the question cannot be answered using the information \n",
    "    provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \n",
    "        Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dedbf3d-e205-4c3c-8f32-ce06e4c08ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face, OpenAI, and Cohere.\n"
     ]
    }
   ],
   "source": [
    "completion = model.invoke(prompt)\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31773e64-4821-4879-bd1d-1dc9988fa35a",
   "metadata": {},
   "source": [
    "If you're building a chat application, you can use `ChatPromptTemplate` to provide dynamic inputs based on the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d8c3a1-d605-46b1-b5e5-b26dc8d75471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hugging Face, OpenAI, and Cohere.', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T18:40:32.453948Z', 'done': True, 'done_reason': 'stop', 'total_duration': 357840333, 'load_duration': 148530667, 'prompt_eval_count': 159, 'prompt_eval_duration': 96736584, 'eval_count': 11, 'eval_duration': 95001166, 'logprobs': None, 'model_name': 'gemma3:1b'}, id='run--f3c6479a-6201-4335-ac16-c86e48a9b40f-0', usage_metadata={'input_tokens': 159, 'output_tokens': 11, 'total_tokens': 170})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Answer the question based on the context below. If the \n",
    "        question cannot be answered using the information provided, answer with \n",
    "        \"I don\\'t know\".'''),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"The most recent advancements in NLP are being driven by \n",
    "        Large Language Models (LLMs). These models outperform their smaller \n",
    "        counterparts and have become invaluable for developers who are creating \n",
    "        applications with NLP capabilities. Developers can tap into these \n",
    "        models through Hugging Face's `transformers` library, or by utilizing \n",
    "        OpenAI and Cohere's offerings through the `openai` and `cohere` \n",
    "        libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd690bff-eb01-441e-aeca-3902e46bec9b",
   "metadata": {},
   "source": [
    "## Getting output in specific format\n",
    "\n",
    "Plain text output are useful, but cannot be used in automation systems.\n",
    "If you want to retrieve answers in JSON format, you can do so by specifying the schema of the output expected. This is where you can use pydantic models to define the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ca3c49a-5ac6-4430-8081-146ebfdeb52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushpatel/IdeaProjects/ai-eng-projects/project_2/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='A pound of feathers weighs more.', justification='A pound is a unit of weight. Therefore, a pound of feathers will always be heavier than a pound of bricks.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the \n",
    "        answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''\n",
    "\n",
    "llm = ChatOllama(model='gemma3:1b', temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "structured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound  of feathers\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1bf6c-87ce-4b88-b34f-b15bf180ef1e",
   "metadata": {},
   "source": [
    "With above code, the schema will validate the output returned by the LLM before actually returning this. Before calling LLM, the schema is converted into JSONSchema and for each LLM, LangChain picks the best method to do this, usually using function calling or prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4e04b-fb60-42ca-b1d9-ff83640aa627",
   "metadata": {},
   "source": [
    "### Getting output in other formats\n",
    "\n",
    "If you want chat model to produce output in other formats (CSV or XML), you cna use output parers. **Output Parsers** are classes that help you structure large language model responses. It will include additional instructions in the prompt that will help guide the LLM to output text in the format it knows how to parse. The other function is to take the text output of the LLM or chat model and render it to a more structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1881677b-e2bd-46d3-9017-ae067dc292d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "items = parser.invoke('apple, banana, cherry')\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c05314-8a95-4ced-a5c7-5489e3bcbaf1",
   "metadata": {},
   "source": [
    "## Combine Components of LangChain\n",
    "\n",
    "The components of the LangChain can be combined to build LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55dae2-7a41-49e8-bcef-39d918cdecdb",
   "metadata": {},
   "source": [
    "### 1. Using Runnable Interface\n",
    "\n",
    "All components usually follow similar interface. They use `invoke()` method to generate output. The common interface includes following methods.\n",
    "\n",
    "1. `invoke`: transform single input to an output.\n",
    "2. `batch`: transform multiple inputs into multiple outputs.\n",
    "3. `stream`: streams output from a single input as it's produced.\n",
    "\n",
    "There are built-in retries, fallbacks, schemas and runtime configurability. In Python, each method have `asyncio` equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a00d56-6b58-4013-a877-a8d70429fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Bye' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' ðŸ‘‹' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' You' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' too' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' Have' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' a' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' great' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' day' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content=' ðŸ˜Š' additional_kwargs={} response_metadata={} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T19:17:28.249602Z', 'done': True, 'done_reason': 'stop', 'total_duration': 278854542, 'load_duration': 149431792, 'prompt_eval_count': 11, 'prompt_eval_duration': 23075833, 'eval_count': 13, 'eval_duration': 101994329, 'logprobs': None, 'model_name': 'gemma3:1b'} id='run--0959e316-b8ad-4f27-9276-4bbacd9d2244' usage_metadata={'input_tokens': 11, 'output_tokens': 13, 'total_tokens': 24}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='gemma3:1b')\n",
    "\n",
    "completion = model.invoke('Hi there')\n",
    "\n",
    "completions = model.batch(['Hi there!', 'Bye!'])\n",
    "\n",
    "for token in model.stream('Bye!'):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac48130-57b5-498b-a400-6f9bc0ddf438",
   "metadata": {},
   "source": [
    "You can combine these components in two ways:\n",
    "1. Imperative: Call each components directly using `model.invoke()`.\n",
    "2. Declarative: Use LangChain Expression Language (LCEL)\n",
    "\n",
    "| | Imperative | Declarative |\n",
    "|:-----------------|:-------------------|:------------------------|\n",
    "| Syntax | All Python or Javascript | LCEL |\n",
    "| Parallel Execution | Python: with threads or coroutines, in JS: with Promise | Automatic |\n",
    "| Streaming | with `yield` keyword | Automatic |\n",
    "| Async execution | with `async` functions | Automatic |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c14e36-80b5-4612-bd26-50bcd8d9fb36",
   "metadata": {},
   "source": [
    "### 2. Imperative Composition\n",
    "\n",
    "This is explicit instructions using Python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab009f53-5d40-49e0-ba6b-691f6c1a5723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, let's break down which model providers offer Large Language Models (LLMs). It's a rapidly evolving landscape, but here's a breakdown of the major players as of late 2024, categorized by their primary focus and offering:\\n\\n**1. Leading Giants - Offering Broad Capabilities & Infrastructure**\\n\\n* **OpenAI:** (GPT Series - GPT-4, GPT-4o) - This is *the* dominant player.  They provide access to GPT models through their API, ChatGPT, and various tools.\\n    * **Strengths:** Extremely versatile, strong in text generation, reasoning, coding, and understanding complex prompts.  GPT-4o is significantly improved in audio and video understanding.\\n    * **Cost:** Paid API access, various tiers based on usage.\\n    * **Accessibility:** Very widely accessible through their API and ChatGPT.\\n* **Google AI (Gemini):** (Gemini Pro, Gemini Ultra, Gemini Nano) - Google is investing heavily in LLMs. Gemini is their flagship offering, boasting impressive multimodal capabilities.\\n    * **Strengths:** Deep integration with Google services (Search, Workspace, etc.). Strong reasoning, coding, and multilingual capabilities. Gemini Ultra is designed for complex tasks.\\n    * **Cost:**  Gemini Pro offers a tiered pricing structure.  Gemini Ultra is a premium service.\\n    * **Accessibility:**  Available through Vertex AI (their platform for AI), Google AI Studio, and through Google's apps and services.\\n* **Anthropic:** (Claude) - Focused on safety and helpfulness. Claude is known for its reasoning and conversational abilities.\\n    * **Strengths:**  Emphasis on safety and reducing harmful outputs. Excellent for creative writing, summarization, and detailed explanations.\\n    * **Cost:** Paid API access.\\n    * **Accessibility:**  Available through API and through Anthropic's tools.\\n\\n\\n**2. Specialized & Emerging Models (Smaller, Focused, or with Unique Strengths)**\\n\\n* **Cohere:** (Command) - Primarily focused on enterprise use cases, emphasizing data privacy and control.\\n    * **Strengths:**  Good for tasks like content generation, summarization, and data extraction. Strong focus on security.\\n    * **Cost:**  Paid API access.\\n* **Meta AI (Llama 2, Llama 3):** (Llama 3) â€“ Meta's open-source models. Llama 3 are particularly competitive.\\n    * **Strengths:**  Open-source nature allows for more customization and research.  Llama 3 is impressive for its performance and safety.\\n    * **Cost:** Free to use, but you'll need compute resources to run the models.\\n    * **Accessibility:** Downloadable and run locally or on cloud infrastructure.\\n* **Mistral AI:** (Mixtral 8x7B, Mistral Large) -  A French company that has released several open-source models that are very competitive.  They're known for their efficiency.\\n    * **Strengths:** Good performance for their size, and an emphasis on open-source development.\\n    * **Cost:** Free to use (with usage limitations) or paid for commercial use.\\n    * **Accessibility:** Can be run locally or through cloud platforms.\\n\\n\\n**3. Other Notable Players**\\n\\n* **AI21 Labs (Jurassic Text):** Specializes in large language models for enterprise applications.\\n* **Hugging Face:** A platform that hosts many open-source LLMs and provides tools for model evaluation and deployment.  Itâ€™s not a provider *per se*, but a crucial ecosystem.\\n\\n\\n**Resources for Staying Up-to-Date:**\\n\\n* **OpenAI Blog:** [https://openai.com/blog/](https://openai.com/blog/)\\n* **Google AI Blog:** [https://ai.googleblog.com/](https://ai.googleblog.com/)\\n* **Anthropic Blog:** [https://blog.anthropic.com/](https://blog.anthropic.com/)\\n* **TechCrunch:** [https://techcrunch.com/](https://techcrunch.com/) - News and analysis of AI developments.\\n\\n**To help me narrow down recommendations for you, could you tell me:**\\n\\n*   **What will you primarily use the LLM for?** (e.g., creative writing, coding assistance, customer service, data analysis, etc.)\\n*   **What's your budget?** (e.g., free, low-cost, willing to pay for premium features)\", additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T19:31:57.197291Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10122718917, 'load_duration': 1004710125, 'prompt_eval_count': 27, 'prompt_eval_duration': 153319416, 'eval_count': 962, 'eval_duration': 8598521313, 'logprobs': None, 'model_name': 'gemma3:1b'}, id='run--2b47af1b-5dae-4e51-a840-f008de8c2a68-0', usage_metadata={'input_tokens': 27, 'output_tokens': 962, 'total_tokens': 989})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# the building blocks\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "model = ChatOllama(model='gemma3:1b', )\n",
    "\n",
    "# combine them in a function\n",
    "# @chain decorator adds the same Runnable interface for any function you write\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# use it\n",
    "\n",
    "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93db6e36-a227-4b4d-b958-d13e62350dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, let\\'s dive into the GPT series! It\\'s a really fascinating and rapidly evolving area of AI, and itâ€™s important to understand where itâ€™s coming from. Hereâ€™s a breakdown, broken down into key aspects:\\n\\n**1. What is GPT? (The Foundation)**\\n\\n* **GPT stands for \"Generative Pre-trained Transformer.\"** It\\'s a type of Large Language Model (LLM). Let\\'s unpack that:\\n    * **Generative:** It *creates* new text â€“ it doesnâ€™t just retrieve information.\\n    * **Pre-trained:** Itâ€™s been trained on a massive dataset of text and code from the internet. This means it learns patterns, relationships, and a vast amount of knowledge about language.\\n    * **Transformer:** This is the underlying neural network architecture. Transformers are particularly good at understanding context and relationships within text, which is crucial for generating coherent and relevant responses. \\n\\n**2. The GPT Series â€“ Key Versions**\\n\\n* **GPT-1 (2018):** The original. It demonstrated the potential of large language models, but was relatively small and limited in its capabilities.\\n* **GPT-2 (2019):** This is where things got *really* interesting. GPT-2 was significantly larger and more powerful than GPT-1. It produced surprisingly coherent and creative text, generating entire articles and even code.  **It also quickly sparked controversy due to concerns about potential misuse.**\\n* **GPT-3 (2020):** A huge leap forward! GPT-3 was trained on an even *larger* dataset and had a substantially improved understanding of language.  It could perform a wider range of tasks, including:\\n    * Writing different kinds of creative content (poems, code, scripts, musical pieces, email, letters, etc.)\\n    * Answering your questions in an informative way\\n    * Following your instructions and completing your requests thoughtfully.\\n    * It excelled at few-shot learning â€“ learning from just a few examples.\\n* **GPT-3.5 (2022):**  An improved version of GPT-3, trained to be better at following instructions and generating more accurate and helpful responses. It\\'s the model used in many ChatGPT versions.\\n* **GPT-4 (2023):** This is the current flagship model.  It represents a *massive* step forward in capabilities.  Hereâ€™s what makes it significant:\\n    * **Multimodal:**  It can accept images *as input* in addition to text!  This opens up a whole new world of possibilities.\\n    * **Improved Reasoning:** Significantly better at complex reasoning, problem-solving, and understanding nuanced instructions.\\n    * **Longer Context Window:** Can handle much longer pieces of text â€“ this allows for better coherence in longer conversations and more effectively using context.\\n    * **More Creative:**  More flexible and better at mimicking different writing styles.\\n    * **Safety Improvements:**  Significant work has been done to reduce harmful outputs and biases.\\n\\n\\n**3. Key Capabilities & How They Work**\\n\\n* **Text Generation:** The core ability â€“ creating text.\\n* **Translation:** Converting text from one language to another.\\n* **Summarization:** Providing concise summaries of longer texts.\\n* **Question Answering:** Answering questions based on provided context.\\n* **Code Generation:** Writing code in various programming languages.\\n* **Creative Writing:** Generating poems, stories, scripts, and other creative content.\\n* **Conversation:** Engaging in natural-sounding conversations.\\n\\n\\n**4.  Important Considerations & Ethical Concerns**\\n\\n* **Bias:** GPT models are trained on data that reflects existing societal biases, so they can sometimes perpetuate harmful stereotypes.\\n* **Hallucinations:**  They can sometimes generate incorrect or nonsensical information â€“ they can \"hallucinate\" facts.\\n* **Misuse Potential:**  The ability to generate realistic text raises concerns about misinformation, plagiarism, and malicious use (e.g., generating fake news).\\n* **Copyright:**  The legal landscape around copyright for text generated by AI is still evolving.\\n\\n**5. Where to Learn More & Explore:**\\n\\n* **OpenAI Website:** [https://openai.com/](https://openai.com/) - The source for all things GPT.\\n* **Hugging Face:** [https://huggingface.co/](https://huggingface.co/) - A platform for sharing and using AI models, including GPT models.\\n* **YouTube:** Search for \"GPT-4 explained\" or \"GPT-3 tutorials\" - there are many helpful visual explanations.\\n\\n---\\n\\n**To help me give you *even more* tailored information, could you tell me:**\\n\\n*   **What specifically are you interested in learning about GPT?** (e.g., are you curious about its training data, its potential applications, or ethical concerns?)\\n*   **What is your current level of understanding of AI/LLMs?** (e.g., are you a complete beginner, or do you have some basic knowledge?)', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T19:34:57.869294Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10556243166, 'load_duration': 174264958, 'prompt_eval_count': 31, 'prompt_eval_duration': 70093375, 'eval_count': 1066, 'eval_duration': 9952016278, 'logprobs': None, 'model_name': 'gemma3:1b'}, id='run--8c6f7b33-dbd1-44fe-ad5c-818f496d82fd-0', usage_metadata={'input_tokens': 31, 'output_tokens': 1066, 'total_tokens': 1097})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.invoke({\"question\": \"Ok, Can you provide more information on GPT series?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05001477-0355-46eb-a12a-46d4f401b3b6",
   "metadata": {},
   "source": [
    "For adding streaming or async support, you'd have to modify the function to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0df329-c0ee-4867-8819-ef198f8841fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "for part in chatbot.stream({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "}):\n",
    "    pass\n",
    "    # print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c0eef-e653-48b3-9d68-080aee1662f1",
   "metadata": {},
   "source": [
    "For async execution, you can update like below.\n",
    "\n",
    "```python\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157221f-7f58-4910-8f94-b30eb6b90611",
   "metadata": {},
   "source": [
    "### 3. Declarative Composition\n",
    "\n",
    "LCEL is declarative language for composing LangChain components. Langchain compiles LCEL compositions to an optimized execution plan, with automatic paralellelization, streaming, tracing and async support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d075eaa9-e657-4e71-9bfd-16176da11193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, let's break down which model providers are currently offering Large Language Models (LLMs). Itâ€™s a rapidly evolving field, but hereâ€™s a breakdown of the major players, categorized by their approach and strengths:\\n\\n**1. The Big Players - Leading the Charge**\\n\\n* **OpenAI:** (GPT series - GPT-4, GPT-3.5, etc.) - Arguably the most well-known. Theyâ€™ve been at the forefront of LLM development for a long time.\\n    * **Strengths:**  Excellent general-purpose models, strong reasoning, creative writing, coding assistance, and a vast ecosystem of tools. GPT-4 is significantly more advanced than previous versions.\\n    * **Pricing:**  Offers a free tier (GPT-3.5), paid subscriptions (ChatGPT Plus, API access), and custom pricing for enterprise use.\\n* **Google (Gemini):** (Gemini models - Ultra, Pro, Nano) - Google is heavily investing in LLMs. Gemini is their flagship model, and itâ€™s designed to be multimodal (understanding text, images, audio, and video).\\n    * **Strengths:**  Strong reasoning, multimodal capabilities, integration with Google services (Search, Workspace, etc.), and impressive performance across various benchmarks. Gemini Ultra is their most advanced model.\\n    * **Pricing:**  Gemini is available through various Google AI Studio and Vertex AI offerings, with different tiers based on usage.\\n* **Anthropic (Claude):** (Claude models) - Focused on safety and helpfulness. Claude is designed to be less prone to generating harmful or biased responses.\\n    * **Strengths:** Excellent at reasoning, long-form content generation, and following complex instructions. Known for its focus on safety.\\n    * **Pricing:**  Offers a free tier and paid subscriptions.\\n\\n**2. Other Notable Providers**\\n\\n* **Meta (Llama 2 & Llama 3):** (Llama 2, Llama 3) - Meta's open-source models are gaining traction. Llama 2 is a powerful and versatile model, while Llama 3 is a significant step forward.\\n    * **Strengths:** Open-source, allowing for greater customization and research.  Llama 3 is particularly strong in reasoning and coding.\\n    * **Pricing:**  Free for research and commercial use (subject to licensing terms).\\n* **Cohere:** (Command, Generate) - Specializes in enterprise-grade LLMs, focusing on text generation, summarization, and search.\\n    * **Strengths:** Strong for business applications, emphasis on data privacy, and API-driven access.\\n    * **Pricing:** Subscription-based, tiered pricing based on usage.\\n* **AI21 Labs (Jurassic-2):** (Jurassic-2) -  Focuses on high-quality text generation and understanding.\\n    * **Strengths:**  Good for creative writing, summarization, and complex text analysis.\\n    * **Pricing:**  Subscription-based.\\n* **Amazon (Bedrock):** (Bedrock) -  Amazon's platform that allows access to multiple LLMs (including those from OpenAI, Anthropic, Meta, and others).\\n    * **Strengths:**  Unified access to a variety of models, integration with AWS services.\\n    * **Pricing:**  Subscription-based, with different tiers based on access to models.\\n\\n\\n**3.  Emerging Players & Specialized Models**\\n\\n* **Mistral AI:** (Mistral 7B, Mixtral 8x7B) -  Known for high performance with relatively small models.\\n* **Hugging Face:** (Various models) - A platform that hosts a vast library of open-source models, making it easier for developers to experiment with LLMs.\\n\\n\\n**Resources for Staying Updated:**\\n\\n* **OpenAI Blog:** [https://openai.com/blog/](https://openai.com/blog/)\\n* **Google AI Blog:** [https://ai.googleblog.com/](https://ai.googleblog.com/)\\n* **Anthropic Blog:** [https://blog.anthropic.com/](https://blog.anthropic.com/)\\n* **TechCrunch:** [https://techcrunch.com/](https://techcrunch.com/) (Good for news and trends)\\n\\n**To help me give you even more tailored information, could you tell me:**\\n\\n*   **What are you planning to use the LLM for?** (e.g., writing, coding, chatbots, research, etc.)\\n*   **What's your budget?** (Are you looking for free models, or are you willing to pay for higher performance?)\", additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-12-12T19:41:56.04687Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9299711542, 'load_duration': 154954125, 'prompt_eval_count': 27, 'prompt_eval_duration': 69894000, 'eval_count': 990, 'eval_duration': 8762187665, 'logprobs': None, 'model_name': 'gemma3:1b'}, id='run--66a5c558-9859-4607-a68a-04ceebfdfea4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 990, 'total_tokens': 1017})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "model = ChatOllama(model='gemma3:1b', temperature=0.1)\n",
    "\n",
    "# combine them with the | operator\n",
    "\n",
    "chatbot = template | model\n",
    "\n",
    "# use it\n",
    "\n",
    "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830e9b4-259e-4242-b0ed-7a0bec6ad3c9",
   "metadata": {},
   "source": [
    "Again, for async invocation, you can use `ainvoke` method.\n",
    "\n",
    "```python\n",
    "await chatbot.ainvoke({\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56b5ad-80ec-42ba-ba31-3aa2d5f1a431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-2",
   "language": "python",
   "name": "project-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
