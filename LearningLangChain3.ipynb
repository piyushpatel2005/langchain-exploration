{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be7523d-171f-45be-94f2-1c1988da5247",
   "metadata": {},
   "source": [
    "# Add Memory to Chatbot\n",
    "\n",
    "In order to make your application chat back and forth with users, you need to store prior conversations and relevant context. LLMs are stateless and hence each time you prompt, a new response is generated. To provide historical information to the model, you need a memory system that can keep track of previous conversations and context.\n",
    "\n",
    "A simple way to build a chatbot memory system is to store and reuse the history of all chat interactions between the user and the model. The state of this system can be stored as a list of messages, updated by appending recent messages after each turn, appeneded into the prompt by inserting the messages into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8816e46b-b854-45d7-919b-27f73c5363d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I translated the sentence \"I love programming\" from English to French as \"J\\'adore programmer.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama import ChatOllama\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import os\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant. Answer all questions to the best \n",
    "        of your ability.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "if 'MISTRAL_API_KEY' not in os.environ:\n",
    "    raise ValueError('MISTRAL_API_KEY required to work with ChatMistralAI')\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "# model = ChatOllama(model='gemma3:latest', temperature=0)\n",
    "model = ChatMistralAI(\n",
    "    model='mistral-small-latest',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\",\"\"\"Translate this sentence from English to French: I love \n",
    "            programming.\"\"\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"What did you just say?\"),\n",
    "    ],\n",
    "})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483a19d-ae03-459b-8530-9f63c69c56ee",
   "metadata": {},
   "source": [
    "Some of the challenges with this approach to store interactions are below.\n",
    "- You need to update memory after each interaction atomically.\n",
    "- You'll have to store these memories in durable storage like RDBMs.\n",
    "- You also have to control how many and which messages are stored for later use and how many of those are used for new interactions.\n",
    "- You'll wan to inspect and modify state outside a call to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4563e-7f45-4d8f-a534-a65d36e36580",
   "metadata": {},
   "source": [
    "## LangGraph Intro\n",
    "\n",
    "LangGraph was designed to enable developers to implement multiactor, multistep, stateful cognitive architectures called graphs. \n",
    "Multiple LLMs or even in combination with web search, we can build complex systems. An application with multiple actors need a coordiantion layer to do these things.\n",
    "- Define the actors involved and how they hand off work to each other (edges of that graph).\n",
    "- Schedule execution of each actor at an appropriate time - in parallel if needed - with deterministic results.\n",
    "\n",
    "When each actor hand off to another, you need to understand back-and-forth between multiple actors. You need to know what order it happens in, how many times each actor is called, etc. Communication across steps require tracking state otherwise when you call the LLM actor the second time, you will get the same result as the first time. It's helpful to pull this state out of each actors and have all actors collaborate on updating a single central state. With single state, you can \n",
    "- snapshot and store the central state during or after each computation\n",
    "- pause and resume execution, which makes it easy to recover from errors.\n",
    "- implement human-in-the-loop controls\n",
    "\n",
    "Each graph is made up of below components.\n",
    "- State: The data received from outside the application, modified and produced by the application while it's running\n",
    "- Nodes: Each step is called a node. Nodes are Python functions which receive the current state as input and can return an updte to that state\n",
    "- Edges: The connection between nodes. Edges determine the path taken from the first node to the last or they can be fixed or conditional.\n",
    "\n",
    "LangGraph offers utilities to visualize these graphs and numerous features to debug their workings while developing them. These graphs can then be deployed to serve production workloads at scale. \n",
    "\n",
    "You can add LangGraph using `uv` with below command.\n",
    "\n",
    "```shell\n",
    "uv add langgraph\n",
    "```\n",
    "\n",
    "### Create StateGraph\n",
    "\n",
    "First, create `StateGraph`.\n",
    "\n",
    "For defining graph, first define the state of the graph. The state consists of the shape, schema of the graph state and a reducer functions which specify how to update the state. In this case, the state is a dictionary with single key `messages`. The `messages` key is annotated with `add_messages` reducer function. This function tells LangGraph to append new messages to the existing list.\n",
    "\n",
    "State keys without an annotation will be overwritten by each update, storing the most recent value. You can also use your own reducer which receive two arguments:\n",
    "1. current state\n",
    "2. The next value being written to the state\n",
    "and it returns the next state which is the result of merging the current state with the new value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ffd622-8c79-471c-99a0-a283aba2a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` \n",
    "    # function in the annotation defines how this state should \n",
    "    # be updated (in this case, it appends new messages to the \n",
    "    # list, rather than replacing the previous messages)\n",
    "\tmessages: Annotated[list, add_messages]\n",
    "\n",
    "builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853f850-9152-461c-893f-ab781c56f141",
   "metadata": {},
   "source": [
    "Every `node` you define will receive the current `State` as input and return a value that updates that state. Next, add `chatbot` node. Nodes represent units of work which are typically just functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf5c8e0-a69f-4709-a40b-ed60dd9f3310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11a9fdc70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# model = ChatOllama(\n",
    "#     model = 'gemma3:latest',\n",
    "#     temperature=0.1\n",
    "# )\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(\n",
    "    model = 'mistral-small-latest',\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state['messages'])\n",
    "    return {'messages': [answer]}\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or Runnable to run\n",
    "builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b01ec4-7d50-4c6b-9cff-2832a81377bf",
   "metadata": {},
   "source": [
    "This node receives current state, calls LLM and returns an update to the state containing the new message by the LLM. Finally, add the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed568bd7-fcf6-485e-bb63-8c97e0e1b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75449dfe-466c-4682-a0b9-2ccb06936b7c",
   "metadata": {},
   "source": [
    "This tells the graph where to start its work each time you run it. This instructs the graph where it should exit (LangChain will stop execution once there's no more nodes to run). It compiles the graph into a runnable object with `invoke` and `stream` methods.\n",
    "\n",
    "You can also draw a visual representation of the graph using below code.\n",
    "\n",
    "```python\n",
    "graph.get_graph().draw_mermaid_png()\n",
    "```\n",
    "\n",
    "You can run it graph with `stream()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "259ea626-9404-4a30-a77a-b8c20e7f99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chatbot': {'messages': [AIMessage(content='Hello! ðŸ˜Š How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 5, 'total_tokens': 18, 'completion_tokens': 13}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--1a6cd2d2-3891-4945-a58e-a267fdcfae9e-0', usage_metadata={'input_tokens': 5, 'output_tokens': 13, 'total_tokens': 18})]}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage('hi!')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c522ede-a70e-4d9b-952e-65fd53e7170b",
   "metadata": {},
   "source": [
    "The graph has input of the same shape as the defined `State` object.\n",
    "\n",
    "## Adding Memory to StateGraph\n",
    "\n",
    "LangGraph has built-in persistence. You will recompile your graph, attaching a checkpointer (which is a storage adapter for LangChain). LangChain ships with a base class that any user can subclass to create an adapter for their favorite database. LangChain also ships with several adapters maintained by LangChain.\n",
    "- An in-memory adapter\n",
    "- SQLite Adapter\n",
    "- Postgres adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dee039-cd0d-4dc3-95b9-1abe37913c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95a2e6-bab2-4b9c-9c81-f877c94aeea0",
   "metadata": {},
   "source": [
    "This returns a runnable objet with same methods, but now it stores the state at the end of each step, so every invocation after the first doesn't start from a blank state. Every time the graph is called, it starts by using the checkpointer to fetch the most recent saved state and combines the new input with the previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89613297-8d26-497f-91ee-9cb83318faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result_1 = graph.invoke(\n",
    "    { \"messages\": [HumanMessage(\"hi, my name is Jack!\")] }, \n",
    "    thread1\n",
    ")\n",
    "result_2 = graph.invoke(\n",
    "    { \"messages\": [HumanMessage(\"what is my name?\")] }, \n",
    "    thread1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609d30d-0589-4961-8425-7b81eaf0e489",
   "metadata": {},
   "source": [
    "In LangChain, each interaction belongs to a particular history of interactions called threads. Threads are created automatically when first used. The threads allow LLM application to be used by multiple users with independent conversations which are never mixed up.\n",
    "\n",
    "First time you call the `chatbot` node with single message, it returns another message and both of these are saved in the state. The second time you execute the graph on same thread, the `chatbot` node is called with three messages.\n",
    "\n",
    "You can inspect and update the state directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d857fa99-6c9f-49a1-a3b5-26b4428eb9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='c52cc6cc-61de-4187-9888-3f1a692b0634'), AIMessage(content='Hi Jack! Nice to meet you. ðŸ˜Š How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 10, 'total_tokens': 29, 'completion_tokens': 19}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--b7b42cb5-a0f3-4dc8-b707-6f30b16f54c3-0', usage_metadata={'input_tokens': 10, 'output_tokens': 19, 'total_tokens': 29}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='86d9411c-b81e-48e3-951b-f76e52d265d3'), AIMessage(content='Your name is **Jack**â€”thatâ€™s what you told me! ðŸ˜Š\\n\\n(Unless youâ€™d like to change it or clarify anything, of course!)', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 36, 'total_tokens': 70, 'completion_tokens': 34}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--4df7ff17-cabb-4f48-b1e0-128d2a1b2361-0', usage_metadata={'input_tokens': 36, 'output_tokens': 34, 'total_tokens': 70})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0e5b2c-6002-6ad4-8004-0d119c533605'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-12-30T19:07:23.732405+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0e5b2c-45bb-6ab8-8003-9cc974ab1231'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61698a2-15ad-4f1f-809d-2fa18c413dde",
   "metadata": {},
   "source": [
    "You can update the state like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b074ec-942b-4632-9a9a-2927200d0460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0e5b2d-0d37-6560-8005-80a66adc0a06'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(thread1, {'messages': [HumanMessage(\"I like LLMs!\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe411347-7021-4c42-9420-a88a254fde99",
   "metadata": {},
   "source": [
    "Above code added one message to the list of messages in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3416157-bcff-4497-852c-4ce71733c51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='c52cc6cc-61de-4187-9888-3f1a692b0634'), AIMessage(content='Hi Jack! Nice to meet you. ðŸ˜Š How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 10, 'total_tokens': 29, 'completion_tokens': 19}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--b7b42cb5-a0f3-4dc8-b707-6f30b16f54c3-0', usage_metadata={'input_tokens': 10, 'output_tokens': 19, 'total_tokens': 29}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='86d9411c-b81e-48e3-951b-f76e52d265d3'), AIMessage(content='Your name is **Jack**â€”thatâ€™s what you told me! ðŸ˜Š\\n\\n(Unless youâ€™d like to change it or clarify anything, of course!)', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 36, 'total_tokens': 70, 'completion_tokens': 34}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop'}, id='run--4df7ff17-cabb-4f48-b1e0-128d2a1b2361-0', usage_metadata={'input_tokens': 36, 'output_tokens': 34, 'total_tokens': 70}), HumanMessage(content='I like LLMs!', additional_kwargs={}, response_metadata={}, id='3a4afc27-2b6e-421c-8e92-8723d278034d')]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0e5b2d-0d37-6560-8005-80a66adc0a06'}}, metadata={'source': 'update', 'step': 5, 'parents': {}}, created_at='2025-12-30T19:07:41.894364+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0e5b2c-6002-6ad4-8004-0d119c533605'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(thread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab832d87-3a75-472b-84d8-192ce2d4ba72",
   "metadata": {},
   "source": [
    "## Modifying Chat History\n",
    "\n",
    "You can modify chat history in three ways: trimming, filtering and merging messages.\n",
    "\n",
    "### 1. Trimming Messages\n",
    "\n",
    "LLMs have limited context windows. The final prompt sent to the model shouldn't exceed the limit of context window. If you have long chat history and want to load them into prompt, you may need to trim messages. LangChain provides built-in `trim_messages` that includes various strategies to do this.\n",
    "\n",
    "Below example retrieves the last `max_tokens` in the list of messages by setting a strategy to `\"last\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11af089a-2954-43fd-b9ab-b8ca322dbd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# from langchain_ollama import ChatOllama\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=25,\n",
    "    strategy=\"last\",\n",
    "    # token_counter=ChatOllama(model=\"gemma3:latest\"),\n",
    "    token_counter=ChatMistralAI(model='mistral-small-latest'),\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203963c0-b2f8-44b2-9ec5-845c8502bd12",
   "metadata": {},
   "source": [
    "The `strategy` controls whether to start from the beginning or the end of the list. The `token_counter` is an LLM which will be used to count tokens using tokenizer appropriate to the model. The parameter `start_on` ensures that we never remove an `AIMessage` without also removing a corresponding `HumanMessage` from the chat history.\n",
    "\n",
    "### 2. Filtering Messages\n",
    "\n",
    "LangChain's `filter_messages` makes it easy to filter the chat history by type, ID or name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d984bbc6-8fab-49af-a333-9d03b2b6b1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "\n",
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35704d-4e90-4ec4-9cf1-0e3b19b55fa3",
   "metadata": {},
   "source": [
    "You can see that this tends to keep `HumanMessage`. Below code, on the other hand, exclude users and IDs and include message types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ad4194-9555-46ba-82f0-918c3f3d25e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "\n",
    "filter_messages(\n",
    "    messages, \n",
    "    include_types=[HumanMessage, AIMessage], \n",
    "    exclude_ids=[\"3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949cd4b-2742-46c0-8228-e1b7589d55de",
   "metadata": {},
   "source": [
    "You could also use `filter_messages` as a declarative syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3010001-bd27-4c91-a6e2-966e05901388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatMistralAI(model='mistral-small-latest')\n",
    "filter_ = filter_messages(exclude_names=['example_user', 'example_assistant'])\n",
    "chain = filter_ | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdb3b4-b96a-47df-a11c-2a4d018028bd",
   "metadata": {},
   "source": [
    "### 3. Merging Consecutive Messages\n",
    "\n",
    "LangChain's `merge_message_runs` makes it easy to merge consecutive messages of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea6507d-867a-4469-a63a-cd1222a917ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyway'], additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just \\n        didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the \\n        office!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage(\n",
    "        [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyway\"),\n",
    "    AIMessage(\n",
    "        '''Well, I guess they thought \"WordRope\" and \"SentenceString\" just \n",
    "        didn\\'t have the same ring to it!'''\n",
    "    ),\n",
    "    AIMessage(\"\"\"Why, he's probably chasing after the last cup of coffee in the \n",
    "        office!\"\"\"),\n",
    "]\n",
    "\n",
    "merge_message_runs(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e32c0395-5907-4f13-95c6-75652891139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatMistralAI(model='mistral-small-latest')\n",
    "merger = merge_message_runs()\n",
    "chain = merger | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11a235-9b81-450d-9f3f-3501a4a36301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-exploration",
   "language": "python",
   "name": "langchain-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
